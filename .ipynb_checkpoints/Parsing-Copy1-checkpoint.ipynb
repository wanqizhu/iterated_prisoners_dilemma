{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Median_score</th>\n",
       "      <th>Cooperation_rating</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Initial_C_rate</th>\n",
       "      <th>CC_rate</th>\n",
       "      <th>CD_rate</th>\n",
       "      <th>DC_rate</th>\n",
       "      <th>DD_rate</th>\n",
       "      <th>CC_to_C_rate</th>\n",
       "      <th>CD_to_C_rate</th>\n",
       "      <th>DC_to_C_rate</th>\n",
       "      <th>DD_to_C_rate</th>\n",
       "      <th>present</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evolved HMM 5</th>\n",
       "      <td>1.200480</td>\n",
       "      <td>2.841197</td>\n",
       "      <td>0.751156</td>\n",
       "      <td>4.697362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697511</td>\n",
       "      <td>0.053645</td>\n",
       "      <td>0.124955</td>\n",
       "      <td>0.123889</td>\n",
       "      <td>0.998325</td>\n",
       "      <td>0.522507</td>\n",
       "      <td>0.051792</td>\n",
       "      <td>0.218408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evolved FSM 16</th>\n",
       "      <td>1.336192</td>\n",
       "      <td>2.845268</td>\n",
       "      <td>0.746335</td>\n",
       "      <td>6.269052</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653651</td>\n",
       "      <td>0.092684</td>\n",
       "      <td>0.157662</td>\n",
       "      <td>0.096003</td>\n",
       "      <td>0.863788</td>\n",
       "      <td>0.566658</td>\n",
       "      <td>0.570173</td>\n",
       "      <td>0.405417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EvolvedLookerUp2_2_2</th>\n",
       "      <td>1.419521</td>\n",
       "      <td>2.838284</td>\n",
       "      <td>0.750285</td>\n",
       "      <td>5.182730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675191</td>\n",
       "      <td>0.075094</td>\n",
       "      <td>0.140749</td>\n",
       "      <td>0.108966</td>\n",
       "      <td>0.877316</td>\n",
       "      <td>0.466973</td>\n",
       "      <td>0.589226</td>\n",
       "      <td>0.815119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evolved FSM 16 Noise 05</th>\n",
       "      <td>1.439287</td>\n",
       "      <td>2.841501</td>\n",
       "      <td>0.744215</td>\n",
       "      <td>3.919619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.712260</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>0.112234</td>\n",
       "      <td>0.143552</td>\n",
       "      <td>0.982910</td>\n",
       "      <td>0.560707</td>\n",
       "      <td>0.463790</td>\n",
       "      <td>0.125542</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSO Gambler 2_2_2</th>\n",
       "      <td>1.698889</td>\n",
       "      <td>2.818984</td>\n",
       "      <td>0.725300</td>\n",
       "      <td>4.679981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653211</td>\n",
       "      <td>0.072089</td>\n",
       "      <td>0.146163</td>\n",
       "      <td>0.128537</td>\n",
       "      <td>0.919371</td>\n",
       "      <td>0.477914</td>\n",
       "      <td>0.539747</td>\n",
       "      <td>0.422777</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evolved ANN</th>\n",
       "      <td>1.710155</td>\n",
       "      <td>2.819515</td>\n",
       "      <td>0.756517</td>\n",
       "      <td>3.973518</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.723002</td>\n",
       "      <td>0.033515</td>\n",
       "      <td>0.101756</td>\n",
       "      <td>0.141726</td>\n",
       "      <td>0.981193</td>\n",
       "      <td>0.740428</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evolved ANN 5</th>\n",
       "      <td>2.285061</td>\n",
       "      <td>2.799168</td>\n",
       "      <td>0.760187</td>\n",
       "      <td>4.007938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724468</td>\n",
       "      <td>0.035719</td>\n",
       "      <td>0.096488</td>\n",
       "      <td>0.143326</td>\n",
       "      <td>0.964456</td>\n",
       "      <td>0.729634</td>\n",
       "      <td>0.217151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSO Gambler 1_1_1</th>\n",
       "      <td>2.288529</td>\n",
       "      <td>2.795169</td>\n",
       "      <td>0.712055</td>\n",
       "      <td>5.141022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670263</td>\n",
       "      <td>0.041792</td>\n",
       "      <td>0.124109</td>\n",
       "      <td>0.163837</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.411130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BackStabber: (D, D)</th>\n",
       "      <td>2.307005</td>\n",
       "      <td>2.785431</td>\n",
       "      <td>0.703130</td>\n",
       "      <td>17.839251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672262</td>\n",
       "      <td>0.030868</td>\n",
       "      <td>0.117944</td>\n",
       "      <td>0.178926</td>\n",
       "      <td>0.985515</td>\n",
       "      <td>0.805194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omega TFT: 3, 8</th>\n",
       "      <td>2.373248</td>\n",
       "      <td>2.784956</td>\n",
       "      <td>0.758561</td>\n",
       "      <td>3.545857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.731191</td>\n",
       "      <td>0.027370</td>\n",
       "      <td>0.087486</td>\n",
       "      <td>0.153953</td>\n",
       "      <td>0.999685</td>\n",
       "      <td>0.245300</td>\n",
       "      <td>0.638885</td>\n",
       "      <td>0.072430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DoubleCrosser: (D, D)</th>\n",
       "      <td>2.398598</td>\n",
       "      <td>2.782237</td>\n",
       "      <td>0.708372</td>\n",
       "      <td>17.439328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675483</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>0.116040</td>\n",
       "      <td>0.175588</td>\n",
       "      <td>0.985030</td>\n",
       "      <td>0.800917</td>\n",
       "      <td>0.301738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fool Me Once</th>\n",
       "      <td>2.402730</td>\n",
       "      <td>2.796797</td>\n",
       "      <td>0.678004</td>\n",
       "      <td>5.408685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660403</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.123398</td>\n",
       "      <td>0.198599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSO Gambler Mem1</th>\n",
       "      <td>2.569205</td>\n",
       "      <td>2.785908</td>\n",
       "      <td>0.738367</td>\n",
       "      <td>4.557772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.688886</td>\n",
       "      <td>0.049481</td>\n",
       "      <td>0.114404</td>\n",
       "      <td>0.147228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.532979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evolved FSM 4</th>\n",
       "      <td>2.691523</td>\n",
       "      <td>2.794481</td>\n",
       "      <td>0.782045</td>\n",
       "      <td>4.070925</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678194</td>\n",
       "      <td>0.103851</td>\n",
       "      <td>0.135486</td>\n",
       "      <td>0.082469</td>\n",
       "      <td>0.942258</td>\n",
       "      <td>0.443834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.801025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winner12</th>\n",
       "      <td>2.719133</td>\n",
       "      <td>2.786755</td>\n",
       "      <td>0.712447</td>\n",
       "      <td>4.384779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.668213</td>\n",
       "      <td>0.044233</td>\n",
       "      <td>0.123640</td>\n",
       "      <td>0.163913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.472112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSO Gambler 2_2_2 Noise 05</th>\n",
       "      <td>2.987023</td>\n",
       "      <td>2.771516</td>\n",
       "      <td>0.752820</td>\n",
       "      <td>4.087169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684847</td>\n",
       "      <td>0.067973</td>\n",
       "      <td>0.117449</td>\n",
       "      <td>0.129731</td>\n",
       "      <td>0.870274</td>\n",
       "      <td>0.505416</td>\n",
       "      <td>0.788688</td>\n",
       "      <td>0.419380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEM2</th>\n",
       "      <td>3.026590</td>\n",
       "      <td>2.776020</td>\n",
       "      <td>0.634318</td>\n",
       "      <td>6.509319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.621763</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.136263</td>\n",
       "      <td>0.229419</td>\n",
       "      <td>0.996510</td>\n",
       "      <td>0.367254</td>\n",
       "      <td>0.390223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DBS: 0.75, 3, 4, 3, 5</th>\n",
       "      <td>3.057544</td>\n",
       "      <td>2.766213</td>\n",
       "      <td>0.780227</td>\n",
       "      <td>3.003627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733041</td>\n",
       "      <td>0.047186</td>\n",
       "      <td>0.086829</td>\n",
       "      <td>0.132944</td>\n",
       "      <td>0.987386</td>\n",
       "      <td>0.809161</td>\n",
       "      <td>0.357674</td>\n",
       "      <td>0.251448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michaelos: (D,)</th>\n",
       "      <td>3.107733</td>\n",
       "      <td>2.761376</td>\n",
       "      <td>0.635155</td>\n",
       "      <td>18.935798</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.620543</td>\n",
       "      <td>0.014612</td>\n",
       "      <td>0.133726</td>\n",
       "      <td>0.231120</td>\n",
       "      <td>0.986084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spiteful Tit For Tat</th>\n",
       "      <td>3.180831</td>\n",
       "      <td>2.762405</td>\n",
       "      <td>0.697165</td>\n",
       "      <td>4.098020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653562</td>\n",
       "      <td>0.043603</td>\n",
       "      <td>0.124721</td>\n",
       "      <td>0.178114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.808891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EugineNier: (D,)</th>\n",
       "      <td>3.243190</td>\n",
       "      <td>2.751149</td>\n",
       "      <td>0.689699</td>\n",
       "      <td>18.788014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.665313</td>\n",
       "      <td>0.024386</td>\n",
       "      <td>0.111228</td>\n",
       "      <td>0.199074</td>\n",
       "      <td>0.985712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta Winner: 8 players</th>\n",
       "      <td>3.263319</td>\n",
       "      <td>2.769041</td>\n",
       "      <td>0.610789</td>\n",
       "      <td>6.841820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.592026</td>\n",
       "      <td>0.018763</td>\n",
       "      <td>0.150938</td>\n",
       "      <td>0.238273</td>\n",
       "      <td>0.968583</td>\n",
       "      <td>0.295591</td>\n",
       "      <td>0.138113</td>\n",
       "      <td>0.067021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EvolvedLookerUp1_1_1</th>\n",
       "      <td>3.451327</td>\n",
       "      <td>2.749944</td>\n",
       "      <td>0.653298</td>\n",
       "      <td>5.982660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608211</td>\n",
       "      <td>0.045087</td>\n",
       "      <td>0.144652</td>\n",
       "      <td>0.202050</td>\n",
       "      <td>0.982034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.918104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forgetful Fool Me Once: 0.05</th>\n",
       "      <td>3.495222</td>\n",
       "      <td>2.739693</td>\n",
       "      <td>0.761296</td>\n",
       "      <td>4.528302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.719463</td>\n",
       "      <td>0.041833</td>\n",
       "      <td>0.085650</td>\n",
       "      <td>0.153054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.643673</td>\n",
       "      <td>0.100576</td>\n",
       "      <td>0.077634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMWE Finite Memory: 86 players</th>\n",
       "      <td>3.639282</td>\n",
       "      <td>2.753409</td>\n",
       "      <td>0.604857</td>\n",
       "      <td>6.855283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588452</td>\n",
       "      <td>0.016405</td>\n",
       "      <td>0.148228</td>\n",
       "      <td>0.246916</td>\n",
       "      <td>0.968419</td>\n",
       "      <td>0.265988</td>\n",
       "      <td>0.080901</td>\n",
       "      <td>0.141585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stein and Rapoport: 0.05: (D, D)</th>\n",
       "      <td>3.745084</td>\n",
       "      <td>2.719625</td>\n",
       "      <td>0.748316</td>\n",
       "      <td>17.998894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704838</td>\n",
       "      <td>0.043477</td>\n",
       "      <td>0.088356</td>\n",
       "      <td>0.163328</td>\n",
       "      <td>0.979382</td>\n",
       "      <td>0.511290</td>\n",
       "      <td>0.565934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMWE Deterministic: 138 players</th>\n",
       "      <td>3.752987</td>\n",
       "      <td>2.751847</td>\n",
       "      <td>0.602318</td>\n",
       "      <td>6.915265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587347</td>\n",
       "      <td>0.014971</td>\n",
       "      <td>0.148031</td>\n",
       "      <td>0.249651</td>\n",
       "      <td>0.974616</td>\n",
       "      <td>0.237255</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>0.105043</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMWE Long Memory: 114 players</th>\n",
       "      <td>3.793628</td>\n",
       "      <td>2.750020</td>\n",
       "      <td>0.603352</td>\n",
       "      <td>6.935795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587496</td>\n",
       "      <td>0.015856</td>\n",
       "      <td>0.147721</td>\n",
       "      <td>0.248927</td>\n",
       "      <td>0.965442</td>\n",
       "      <td>0.342833</td>\n",
       "      <td>0.068507</td>\n",
       "      <td>0.106038</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evolved ANN 5 Noise 05</th>\n",
       "      <td>3.799902</td>\n",
       "      <td>2.739341</td>\n",
       "      <td>0.820153</td>\n",
       "      <td>2.768391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754546</td>\n",
       "      <td>0.065607</td>\n",
       "      <td>0.073964</td>\n",
       "      <td>0.105883</td>\n",
       "      <td>0.984202</td>\n",
       "      <td>0.859179</td>\n",
       "      <td>0.683188</td>\n",
       "      <td>0.275451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nice Meta Winner Ensemble: 200 players</th>\n",
       "      <td>3.813648</td>\n",
       "      <td>2.749795</td>\n",
       "      <td>0.606051</td>\n",
       "      <td>6.826414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589716</td>\n",
       "      <td>0.016335</td>\n",
       "      <td>0.146674</td>\n",
       "      <td>0.247274</td>\n",
       "      <td>0.966637</td>\n",
       "      <td>0.326982</td>\n",
       "      <td>0.071031</td>\n",
       "      <td>0.126515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ThueMorseInverse</th>\n",
       "      <td>19.228784</td>\n",
       "      <td>2.027108</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.605831</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.188728</td>\n",
       "      <td>0.311272</td>\n",
       "      <td>0.240231</td>\n",
       "      <td>0.259769</td>\n",
       "      <td>0.295488</td>\n",
       "      <td>0.400034</td>\n",
       "      <td>0.586173</td>\n",
       "      <td>0.776740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pun1</th>\n",
       "      <td>19.334236</td>\n",
       "      <td>2.075770</td>\n",
       "      <td>0.742908</td>\n",
       "      <td>6.225695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.276999</td>\n",
       "      <td>0.105237</td>\n",
       "      <td>0.151855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390388</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cycler CCCCCD</th>\n",
       "      <td>19.492927</td>\n",
       "      <td>2.067706</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>6.655598</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.496736</td>\n",
       "      <td>0.343264</td>\n",
       "      <td>0.104374</td>\n",
       "      <td>0.055626</td>\n",
       "      <td>0.797144</td>\n",
       "      <td>0.870773</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cycler CCD</th>\n",
       "      <td>19.538009</td>\n",
       "      <td>2.028101</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>6.765326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.322226</td>\n",
       "      <td>0.357774</td>\n",
       "      <td>0.185356</td>\n",
       "      <td>0.134644</td>\n",
       "      <td>0.447850</td>\n",
       "      <td>0.648237</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Defector</th>\n",
       "      <td>19.561016</td>\n",
       "      <td>1.934086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.681401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233522</td>\n",
       "      <td>0.766478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random: 0.3</th>\n",
       "      <td>19.596440</td>\n",
       "      <td>1.988106</td>\n",
       "      <td>0.300031</td>\n",
       "      <td>10.821385</td>\n",
       "      <td>0.298873</td>\n",
       "      <td>0.104265</td>\n",
       "      <td>0.195766</td>\n",
       "      <td>0.243836</td>\n",
       "      <td>0.456133</td>\n",
       "      <td>0.329459</td>\n",
       "      <td>0.306281</td>\n",
       "      <td>0.323607</td>\n",
       "      <td>0.312181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SelfSteem</th>\n",
       "      <td>19.597226</td>\n",
       "      <td>2.022484</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>8.819542</td>\n",
       "      <td>0.500884</td>\n",
       "      <td>0.266978</td>\n",
       "      <td>0.212189</td>\n",
       "      <td>0.175179</td>\n",
       "      <td>0.345654</td>\n",
       "      <td>0.714186</td>\n",
       "      <td>0.402974</td>\n",
       "      <td>0.608069</td>\n",
       "      <td>0.288449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tricky Defector</th>\n",
       "      <td>19.682126</td>\n",
       "      <td>1.928220</td>\n",
       "      <td>0.385802</td>\n",
       "      <td>10.734144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057418</td>\n",
       "      <td>0.328384</td>\n",
       "      <td>0.285442</td>\n",
       "      <td>0.328756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Desperate</th>\n",
       "      <td>19.714819</td>\n",
       "      <td>1.952238</td>\n",
       "      <td>0.359145</td>\n",
       "      <td>9.180075</td>\n",
       "      <td>0.501919</td>\n",
       "      <td>0.058571</td>\n",
       "      <td>0.300574</td>\n",
       "      <td>0.283918</td>\n",
       "      <td>0.356937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Win-Shift Lose-Stay: D</th>\n",
       "      <td>19.721248</td>\n",
       "      <td>1.976207</td>\n",
       "      <td>0.571281</td>\n",
       "      <td>9.632675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214317</td>\n",
       "      <td>0.356964</td>\n",
       "      <td>0.226134</td>\n",
       "      <td>0.202585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cycler CCCD</th>\n",
       "      <td>19.749756</td>\n",
       "      <td>2.041327</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>6.650488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.404850</td>\n",
       "      <td>0.355150</td>\n",
       "      <td>0.146694</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>0.650798</td>\n",
       "      <td>0.770776</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bush Mosteller: 0.5, 0.5, 3.0, 0.5</th>\n",
       "      <td>19.755561</td>\n",
       "      <td>1.987937</td>\n",
       "      <td>0.347478</td>\n",
       "      <td>10.969778</td>\n",
       "      <td>0.499894</td>\n",
       "      <td>0.153339</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.218850</td>\n",
       "      <td>0.433672</td>\n",
       "      <td>0.485907</td>\n",
       "      <td>0.308501</td>\n",
       "      <td>0.363594</td>\n",
       "      <td>0.341624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cycler DC</th>\n",
       "      <td>19.792498</td>\n",
       "      <td>1.994873</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.327963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146410</td>\n",
       "      <td>0.353590</td>\n",
       "      <td>0.263911</td>\n",
       "      <td>0.236089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random: 0.5</th>\n",
       "      <td>19.833130</td>\n",
       "      <td>2.010459</td>\n",
       "      <td>0.499956</td>\n",
       "      <td>8.891230</td>\n",
       "      <td>0.499838</td>\n",
       "      <td>0.215656</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.215862</td>\n",
       "      <td>0.284182</td>\n",
       "      <td>0.497961</td>\n",
       "      <td>0.504710</td>\n",
       "      <td>0.513804</td>\n",
       "      <td>0.516803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta Minority: 200 players</th>\n",
       "      <td>19.868143</td>\n",
       "      <td>1.958364</td>\n",
       "      <td>0.593340</td>\n",
       "      <td>8.673432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151995</td>\n",
       "      <td>0.441344</td>\n",
       "      <td>0.273929</td>\n",
       "      <td>0.132731</td>\n",
       "      <td>0.293255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.998763</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bully</th>\n",
       "      <td>20.003706</td>\n",
       "      <td>1.942125</td>\n",
       "      <td>0.575301</td>\n",
       "      <td>8.898814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131754</td>\n",
       "      <td>0.443547</td>\n",
       "      <td>0.280541</td>\n",
       "      <td>0.144158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cycler DDC</th>\n",
       "      <td>20.005674</td>\n",
       "      <td>1.957048</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>11.233452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082202</td>\n",
       "      <td>0.237798</td>\n",
       "      <td>0.257611</td>\n",
       "      <td>0.422389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428263</td>\n",
       "      <td>0.622247</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ThueMorse</th>\n",
       "      <td>20.020165</td>\n",
       "      <td>1.999828</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.676506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180835</td>\n",
       "      <td>0.319165</td>\n",
       "      <td>0.239331</td>\n",
       "      <td>0.260669</td>\n",
       "      <td>0.320277</td>\n",
       "      <td>0.394073</td>\n",
       "      <td>0.688943</td>\n",
       "      <td>0.755469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Handshake</th>\n",
       "      <td>20.198507</td>\n",
       "      <td>1.911175</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>19.526866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.063344</td>\n",
       "      <td>0.219457</td>\n",
       "      <td>0.668854</td>\n",
       "      <td>0.749524</td>\n",
       "      <td>0.917691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.711122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negation</th>\n",
       "      <td>20.399245</td>\n",
       "      <td>1.908759</td>\n",
       "      <td>0.588633</td>\n",
       "      <td>8.537518</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.140288</td>\n",
       "      <td>0.448346</td>\n",
       "      <td>0.269132</td>\n",
       "      <td>0.142234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cooperator Hunter</th>\n",
       "      <td>20.495222</td>\n",
       "      <td>1.993718</td>\n",
       "      <td>0.924855</td>\n",
       "      <td>4.092212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.556323</td>\n",
       "      <td>0.368532</td>\n",
       "      <td>0.062401</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.944049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SolutionB1</th>\n",
       "      <td>20.720708</td>\n",
       "      <td>1.979532</td>\n",
       "      <td>0.897194</td>\n",
       "      <td>4.389527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590058</td>\n",
       "      <td>0.307136</td>\n",
       "      <td>0.026638</td>\n",
       "      <td>0.076168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479068</td>\n",
       "      <td>0.922667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hopeless</th>\n",
       "      <td>20.745792</td>\n",
       "      <td>1.921248</td>\n",
       "      <td>0.736255</td>\n",
       "      <td>7.891071</td>\n",
       "      <td>0.499615</td>\n",
       "      <td>0.258360</td>\n",
       "      <td>0.477895</td>\n",
       "      <td>0.220606</td>\n",
       "      <td>0.043140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tricky Cooperator</th>\n",
       "      <td>20.748060</td>\n",
       "      <td>1.948405</td>\n",
       "      <td>0.828462</td>\n",
       "      <td>6.097114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.410164</td>\n",
       "      <td>0.418298</td>\n",
       "      <td>0.136594</td>\n",
       "      <td>0.034944</td>\n",
       "      <td>0.877784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anti Tit For Tat</th>\n",
       "      <td>20.758872</td>\n",
       "      <td>1.884351</td>\n",
       "      <td>0.600706</td>\n",
       "      <td>8.371130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.146529</td>\n",
       "      <td>0.454177</td>\n",
       "      <td>0.261368</td>\n",
       "      <td>0.137927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AntiCycler</th>\n",
       "      <td>20.799610</td>\n",
       "      <td>1.961096</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>6.346482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.424296</td>\n",
       "      <td>0.375704</td>\n",
       "      <td>0.122052</td>\n",
       "      <td>0.077948</td>\n",
       "      <td>0.713340</td>\n",
       "      <td>0.788176</td>\n",
       "      <td>0.849694</td>\n",
       "      <td>0.974108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$e$</th>\n",
       "      <td>20.814815</td>\n",
       "      <td>1.931926</td>\n",
       "      <td>0.779289</td>\n",
       "      <td>7.927997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.317275</td>\n",
       "      <td>0.462015</td>\n",
       "      <td>0.189848</td>\n",
       "      <td>0.030863</td>\n",
       "      <td>0.592185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.691790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$\\pi$</th>\n",
       "      <td>20.897374</td>\n",
       "      <td>1.931620</td>\n",
       "      <td>0.799769</td>\n",
       "      <td>6.248784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.338659</td>\n",
       "      <td>0.461110</td>\n",
       "      <td>0.178853</td>\n",
       "      <td>0.021378</td>\n",
       "      <td>0.628025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.791274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradual Killer: (D, D, D, D, D, C, C)</th>\n",
       "      <td>21.179297</td>\n",
       "      <td>1.914904</td>\n",
       "      <td>0.534292</td>\n",
       "      <td>7.984102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390067</td>\n",
       "      <td>0.144225</td>\n",
       "      <td>0.069749</td>\n",
       "      <td>0.395959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771494</td>\n",
       "      <td>0.201799</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$\\phi$</th>\n",
       "      <td>21.224909</td>\n",
       "      <td>1.869301</td>\n",
       "      <td>0.699860</td>\n",
       "      <td>6.775574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230228</td>\n",
       "      <td>0.469632</td>\n",
       "      <td>0.219620</td>\n",
       "      <td>0.080521</td>\n",
       "      <td>0.453695</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.305065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Rank  Median_score  \\\n",
       "Name                                                              \n",
       "Evolved HMM 5                            1.200480      2.841197   \n",
       "Evolved FSM 16                           1.336192      2.845268   \n",
       "EvolvedLookerUp2_2_2                     1.419521      2.838284   \n",
       "Evolved FSM 16 Noise 05                  1.439287      2.841501   \n",
       "PSO Gambler 2_2_2                        1.698889      2.818984   \n",
       "Evolved ANN                              1.710155      2.819515   \n",
       "Evolved ANN 5                            2.285061      2.799168   \n",
       "PSO Gambler 1_1_1                        2.288529      2.795169   \n",
       "BackStabber: (D, D)                      2.307005      2.785431   \n",
       "Omega TFT: 3, 8                          2.373248      2.784956   \n",
       "DoubleCrosser: (D, D)                    2.398598      2.782237   \n",
       "Fool Me Once                             2.402730      2.796797   \n",
       "PSO Gambler Mem1                         2.569205      2.785908   \n",
       "Evolved FSM 4                            2.691523      2.794481   \n",
       "Winner12                                 2.719133      2.786755   \n",
       "PSO Gambler 2_2_2 Noise 05               2.987023      2.771516   \n",
       "MEM2                                     3.026590      2.776020   \n",
       "DBS: 0.75, 3, 4, 3, 5                    3.057544      2.766213   \n",
       "Michaelos: (D,)                          3.107733      2.761376   \n",
       "Spiteful Tit For Tat                     3.180831      2.762405   \n",
       "EugineNier: (D,)                         3.243190      2.751149   \n",
       "Meta Winner: 8 players                   3.263319      2.769041   \n",
       "EvolvedLookerUp1_1_1                     3.451327      2.749944   \n",
       "Forgetful Fool Me Once: 0.05             3.495222      2.739693   \n",
       "NMWE Finite Memory: 86 players           3.639282      2.753409   \n",
       "Stein and Rapoport: 0.05: (D, D)         3.745084      2.719625   \n",
       "NMWE Deterministic: 138 players          3.752987      2.751847   \n",
       "NMWE Long Memory: 114 players            3.793628      2.750020   \n",
       "Evolved ANN 5 Noise 05                   3.799902      2.739341   \n",
       "Nice Meta Winner Ensemble: 200 players   3.813648      2.749795   \n",
       "...                                           ...           ...   \n",
       "ThueMorseInverse                        19.228784      2.027108   \n",
       "Pun1                                    19.334236      2.075770   \n",
       "Cycler CCCCCD                           19.492927      2.067706   \n",
       "Cycler CCD                              19.538009      2.028101   \n",
       "Defector                                19.561016      1.934086   \n",
       "Random: 0.3                             19.596440      1.988106   \n",
       "SelfSteem                               19.597226      2.022484   \n",
       "Tricky Defector                         19.682126      1.928220   \n",
       "Desperate                               19.714819      1.952238   \n",
       "Win-Shift Lose-Stay: D                  19.721248      1.976207   \n",
       "Cycler CCCD                             19.749756      2.041327   \n",
       "Bush Mosteller: 0.5, 0.5, 3.0, 0.5      19.755561      1.987937   \n",
       "Cycler DC                               19.792498      1.994873   \n",
       "Random: 0.5                             19.833130      2.010459   \n",
       "Meta Minority: 200 players              19.868143      1.958364   \n",
       "Bully                                   20.003706      1.942125   \n",
       "Cycler DDC                              20.005674      1.957048   \n",
       "ThueMorse                               20.020165      1.999828   \n",
       "Handshake                               20.198507      1.911175   \n",
       "Negation                                20.399245      1.908759   \n",
       "Cooperator Hunter                       20.495222      1.993718   \n",
       "SolutionB1                              20.720708      1.979532   \n",
       "Hopeless                                20.745792      1.921248   \n",
       "Tricky Cooperator                       20.748060      1.948405   \n",
       "Anti Tit For Tat                        20.758872      1.884351   \n",
       "AntiCycler                              20.799610      1.961096   \n",
       "$e$                                     20.814815      1.931926   \n",
       "$\\pi$                                   20.897374      1.931620   \n",
       "Gradual Killer: (D, D, D, D, D, C, C)   21.179297      1.914904   \n",
       "$\\phi$                                  21.224909      1.869301   \n",
       "\n",
       "                                        Cooperation_rating       Wins  \\\n",
       "Name                                                                    \n",
       "Evolved HMM 5                                     0.751156   4.697362   \n",
       "Evolved FSM 16                                    0.746335   6.269052   \n",
       "EvolvedLookerUp2_2_2                              0.750285   5.182730   \n",
       "Evolved FSM 16 Noise 05                           0.744215   3.919619   \n",
       "PSO Gambler 2_2_2                                 0.725300   4.679981   \n",
       "Evolved ANN                                       0.756517   3.973518   \n",
       "Evolved ANN 5                                     0.760187   4.007938   \n",
       "PSO Gambler 1_1_1                                 0.712055   5.141022   \n",
       "BackStabber: (D, D)                               0.703130  17.839251   \n",
       "Omega TFT: 3, 8                                   0.758561   3.545857   \n",
       "DoubleCrosser: (D, D)                             0.708372  17.439328   \n",
       "Fool Me Once                                      0.678004   5.408685   \n",
       "PSO Gambler Mem1                                  0.738367   4.557772   \n",
       "Evolved FSM 4                                     0.782045   4.070925   \n",
       "Winner12                                          0.712447   4.384779   \n",
       "PSO Gambler 2_2_2 Noise 05                        0.752820   4.087169   \n",
       "MEM2                                              0.634318   6.509319   \n",
       "DBS: 0.75, 3, 4, 3, 5                             0.780227   3.003627   \n",
       "Michaelos: (D,)                                   0.635155  18.935798   \n",
       "Spiteful Tit For Tat                              0.697165   4.098020   \n",
       "EugineNier: (D,)                                  0.689699  18.788014   \n",
       "Meta Winner: 8 players                            0.610789   6.841820   \n",
       "EvolvedLookerUp1_1_1                              0.653298   5.982660   \n",
       "Forgetful Fool Me Once: 0.05                      0.761296   4.528302   \n",
       "NMWE Finite Memory: 86 players                    0.604857   6.855283   \n",
       "Stein and Rapoport: 0.05: (D, D)                  0.748316  17.998894   \n",
       "NMWE Deterministic: 138 players                   0.602318   6.915265   \n",
       "NMWE Long Memory: 114 players                     0.603352   6.935795   \n",
       "Evolved ANN 5 Noise 05                            0.820153   2.768391   \n",
       "Nice Meta Winner Ensemble: 200 players            0.606051   6.826414   \n",
       "...                                                    ...        ...   \n",
       "ThueMorseInverse                                  0.500000   9.605831   \n",
       "Pun1                                              0.742908   6.225695   \n",
       "Cycler CCCCCD                                     0.840000   6.655598   \n",
       "Cycler CCD                                        0.680000   6.765326   \n",
       "Defector                                          0.000000  21.681401   \n",
       "Random: 0.3                                       0.300031  10.821385   \n",
       "SelfSteem                                         0.479167   8.819542   \n",
       "Tricky Defector                                   0.385802  10.734144   \n",
       "Desperate                                         0.359145   9.180075   \n",
       "Win-Shift Lose-Stay: D                            0.571281   9.632675   \n",
       "Cycler CCCD                                       0.760000   6.650488   \n",
       "Bush Mosteller: 0.5, 0.5, 3.0, 0.5                0.347478  10.969778   \n",
       "Cycler DC                                         0.500000   7.327963   \n",
       "Random: 0.5                                       0.499956   8.891230   \n",
       "Meta Minority: 200 players                        0.593340   8.673432   \n",
       "Bully                                             0.575301   8.898814   \n",
       "Cycler DDC                                        0.320000  11.233452   \n",
       "ThueMorse                                         0.500000   7.676506   \n",
       "Handshake                                         0.111690  19.526866   \n",
       "Negation                                          0.588633   8.537518   \n",
       "Cooperator Hunter                                 0.924855   4.092212   \n",
       "SolutionB1                                        0.897194   4.389527   \n",
       "Hopeless                                          0.736255   7.891071   \n",
       "Tricky Cooperator                                 0.828462   6.097114   \n",
       "Anti Tit For Tat                                  0.600706   8.371130   \n",
       "AntiCycler                                        0.800000   6.346482   \n",
       "$e$                                               0.779289   7.927997   \n",
       "$\\pi$                                             0.799769   6.248784   \n",
       "Gradual Killer: (D, D, D, D, D, C, C)             0.534292   7.984102   \n",
       "$\\phi$                                            0.699860   6.775574   \n",
       "\n",
       "                                        Initial_C_rate   CC_rate   CD_rate  \\\n",
       "Name                                                                         \n",
       "Evolved HMM 5                                 1.000000  0.697511  0.053645   \n",
       "Evolved FSM 16                                1.000000  0.653651  0.092684   \n",
       "EvolvedLookerUp2_2_2                          1.000000  0.675191  0.075094   \n",
       "Evolved FSM 16 Noise 05                       1.000000  0.712260  0.031954   \n",
       "PSO Gambler 2_2_2                             1.000000  0.653211  0.072089   \n",
       "Evolved ANN                                   1.000000  0.723002  0.033515   \n",
       "Evolved ANN 5                                 1.000000  0.724468  0.035719   \n",
       "PSO Gambler 1_1_1                             1.000000  0.670263  0.041792   \n",
       "BackStabber: (D, D)                           1.000000  0.672262  0.030868   \n",
       "Omega TFT: 3, 8                               1.000000  0.731191  0.027370   \n",
       "DoubleCrosser: (D, D)                         1.000000  0.675483  0.032889   \n",
       "Fool Me Once                                  1.000000  0.660403  0.017600   \n",
       "PSO Gambler Mem1                              1.000000  0.688886  0.049481   \n",
       "Evolved FSM 4                                 1.000000  0.678194  0.103851   \n",
       "Winner12                                      1.000000  0.668213  0.044233   \n",
       "PSO Gambler 2_2_2 Noise 05                    1.000000  0.684847  0.067973   \n",
       "MEM2                                          1.000000  0.621763  0.012556   \n",
       "DBS: 0.75, 3, 4, 3, 5                         1.000000  0.733041  0.047186   \n",
       "Michaelos: (D,)                               1.000000  0.620543  0.014612   \n",
       "Spiteful Tit For Tat                          1.000000  0.653562  0.043603   \n",
       "EugineNier: (D,)                              1.000000  0.665313  0.024386   \n",
       "Meta Winner: 8 players                        1.000000  0.592026  0.018763   \n",
       "EvolvedLookerUp1_1_1                          1.000000  0.608211  0.045087   \n",
       "Forgetful Fool Me Once: 0.05                  1.000000  0.719463  0.041833   \n",
       "NMWE Finite Memory: 86 players                1.000000  0.588452  0.016405   \n",
       "Stein and Rapoport: 0.05: (D, D)              1.000000  0.704838  0.043477   \n",
       "NMWE Deterministic: 138 players               1.000000  0.587347  0.014971   \n",
       "NMWE Long Memory: 114 players                 1.000000  0.587496  0.015856   \n",
       "Evolved ANN 5 Noise 05                        1.000000  0.754546  0.065607   \n",
       "Nice Meta Winner Ensemble: 200 players        1.000000  0.589716  0.016335   \n",
       "...                                                ...       ...       ...   \n",
       "ThueMorseInverse                              1.000000  0.188728  0.311272   \n",
       "Pun1                                          0.000000  0.465909  0.276999   \n",
       "Cycler CCCCCD                                 1.000000  0.496736  0.343264   \n",
       "Cycler CCD                                    1.000000  0.322226  0.357774   \n",
       "Defector                                      0.000000  0.000000  0.000000   \n",
       "Random: 0.3                                   0.298873  0.104265  0.195766   \n",
       "SelfSteem                                     0.500884  0.266978  0.212189   \n",
       "Tricky Defector                               0.000000  0.057418  0.328384   \n",
       "Desperate                                     0.501919  0.058571  0.300574   \n",
       "Win-Shift Lose-Stay: D                        0.000000  0.214317  0.356964   \n",
       "Cycler CCCD                                   1.000000  0.404850  0.355150   \n",
       "Bush Mosteller: 0.5, 0.5, 3.0, 0.5            0.499894  0.153339  0.194140   \n",
       "Cycler DC                                     0.000000  0.146410  0.353590   \n",
       "Random: 0.5                                   0.499838  0.215656  0.284300   \n",
       "Meta Minority: 200 players                    0.000000  0.151995  0.441344   \n",
       "Bully                                         0.000000  0.131754  0.443547   \n",
       "Cycler DDC                                    0.000000  0.082202  0.237798   \n",
       "ThueMorse                                     0.000000  0.180835  0.319165   \n",
       "Handshake                                     1.000000  0.048346  0.063344   \n",
       "Negation                                      0.499500  0.140288  0.448346   \n",
       "Cooperator Hunter                             1.000000  0.556323  0.368532   \n",
       "SolutionB1                                    0.000000  0.590058  0.307136   \n",
       "Hopeless                                      0.499615  0.258360  0.477895   \n",
       "Tricky Cooperator                             1.000000  0.410164  0.418298   \n",
       "Anti Tit For Tat                              1.000000  0.146529  0.454177   \n",
       "AntiCycler                                    1.000000  0.424296  0.375704   \n",
       "$e$                                           1.000000  0.317275  0.462015   \n",
       "$\\pi$                                         1.000000  0.338659  0.461110   \n",
       "Gradual Killer: (D, D, D, D, D, C, C)         0.000000  0.390067  0.144225   \n",
       "$\\phi$                                        1.000000  0.230228  0.469632   \n",
       "\n",
       "                                         DC_rate   DD_rate  CC_to_C_rate  \\\n",
       "Name                                                                       \n",
       "Evolved HMM 5                           0.124955  0.123889      0.998325   \n",
       "Evolved FSM 16                          0.157662  0.096003      0.863788   \n",
       "EvolvedLookerUp2_2_2                    0.140749  0.108966      0.877316   \n",
       "Evolved FSM 16 Noise 05                 0.112234  0.143552      0.982910   \n",
       "PSO Gambler 2_2_2                       0.146163  0.128537      0.919371   \n",
       "Evolved ANN                             0.101756  0.141726      0.981193   \n",
       "Evolved ANN 5                           0.096488  0.143326      0.964456   \n",
       "PSO Gambler 1_1_1                       0.124109  0.163837      1.000000   \n",
       "BackStabber: (D, D)                     0.117944  0.178926      0.985515   \n",
       "Omega TFT: 3, 8                         0.087486  0.153953      0.999685   \n",
       "DoubleCrosser: (D, D)                   0.116040  0.175588      0.985030   \n",
       "Fool Me Once                            0.123398  0.198599      1.000000   \n",
       "PSO Gambler Mem1                        0.114404  0.147228      1.000000   \n",
       "Evolved FSM 4                           0.135486  0.082469      0.942258   \n",
       "Winner12                                0.123640  0.163913      1.000000   \n",
       "PSO Gambler 2_2_2 Noise 05              0.117449  0.129731      0.870274   \n",
       "MEM2                                    0.136263  0.229419      0.996510   \n",
       "DBS: 0.75, 3, 4, 3, 5                   0.086829  0.132944      0.987386   \n",
       "Michaelos: (D,)                         0.133726  0.231120      0.986084   \n",
       "Spiteful Tit For Tat                    0.124721  0.178114      1.000000   \n",
       "EugineNier: (D,)                        0.111228  0.199074      0.985712   \n",
       "Meta Winner: 8 players                  0.150938  0.238273      0.968583   \n",
       "EvolvedLookerUp1_1_1                    0.144652  0.202050      0.982034   \n",
       "Forgetful Fool Me Once: 0.05            0.085650  0.153054      1.000000   \n",
       "NMWE Finite Memory: 86 players          0.148228  0.246916      0.968419   \n",
       "Stein and Rapoport: 0.05: (D, D)        0.088356  0.163328      0.979382   \n",
       "NMWE Deterministic: 138 players         0.148031  0.249651      0.974616   \n",
       "NMWE Long Memory: 114 players           0.147721  0.248927      0.965442   \n",
       "Evolved ANN 5 Noise 05                  0.073964  0.105883      0.984202   \n",
       "Nice Meta Winner Ensemble: 200 players  0.146674  0.247274      0.966637   \n",
       "...                                          ...       ...           ...   \n",
       "ThueMorseInverse                        0.240231  0.259769      0.295488   \n",
       "Pun1                                    0.105237  0.151855      1.000000   \n",
       "Cycler CCCCCD                           0.104374  0.055626      0.797144   \n",
       "Cycler CCD                              0.185356  0.134644      0.447850   \n",
       "Defector                                0.233522  0.766478      0.000000   \n",
       "Random: 0.3                             0.243836  0.456133      0.329459   \n",
       "SelfSteem                               0.175179  0.345654      0.714186   \n",
       "Tricky Defector                         0.285442  0.328756      0.000000   \n",
       "Desperate                               0.283918  0.356937      0.000000   \n",
       "Win-Shift Lose-Stay: D                  0.226134  0.202585      0.000000   \n",
       "Cycler CCCD                             0.146694  0.093306      0.650798   \n",
       "Bush Mosteller: 0.5, 0.5, 3.0, 0.5      0.218850  0.433672      0.485907   \n",
       "Cycler DC                               0.263911  0.236089      0.000000   \n",
       "Random: 0.5                             0.215862  0.284182      0.497961   \n",
       "Meta Minority: 200 players              0.273929  0.132731      0.293255   \n",
       "Bully                                   0.280541  0.144158      0.000000   \n",
       "Cycler DDC                              0.257611  0.422389      0.000000   \n",
       "ThueMorse                               0.239331  0.260669      0.320277   \n",
       "Handshake                               0.219457  0.668854      0.749524   \n",
       "Negation                                0.269132  0.142234      0.000000   \n",
       "Cooperator Hunter                       0.062401  0.012744      0.944049   \n",
       "SolutionB1                              0.026638  0.076168      1.000000   \n",
       "Hopeless                                0.220606  0.043140      0.000000   \n",
       "Tricky Cooperator                       0.136594  0.034944      0.877784   \n",
       "Anti Tit For Tat                        0.261368  0.137927      0.000000   \n",
       "AntiCycler                              0.122052  0.077948      0.713340   \n",
       "$e$                                     0.189848  0.030863      0.592185   \n",
       "$\\pi$                                   0.178853  0.021378      0.628025   \n",
       "Gradual Killer: (D, D, D, D, D, C, C)   0.069749  0.395959      1.000000   \n",
       "$\\phi$                                  0.219620  0.080521      0.453695   \n",
       "\n",
       "                                        CD_to_C_rate  DC_to_C_rate  \\\n",
       "Name                                                                 \n",
       "Evolved HMM 5                               0.522507      0.051792   \n",
       "Evolved FSM 16                              0.566658      0.570173   \n",
       "EvolvedLookerUp2_2_2                        0.466973      0.589226   \n",
       "Evolved FSM 16 Noise 05                     0.560707      0.463790   \n",
       "PSO Gambler 2_2_2                           0.477914      0.539747   \n",
       "Evolved ANN                                 0.740428      0.446310   \n",
       "Evolved ANN 5                               0.729634      0.217151   \n",
       "PSO Gambler 1_1_1                           0.411130      0.000000   \n",
       "BackStabber: (D, D)                         0.805194      0.000000   \n",
       "Omega TFT: 3, 8                             0.245300      0.638885   \n",
       "DoubleCrosser: (D, D)                       0.800917      0.301738   \n",
       "Fool Me Once                                0.583755      0.000000   \n",
       "PSO Gambler Mem1                            0.532979      0.000000   \n",
       "Evolved FSM 4                               0.443834      0.000000   \n",
       "Winner12                                    0.472112      0.000000   \n",
       "PSO Gambler 2_2_2 Noise 05                  0.505416      0.788688   \n",
       "MEM2                                        0.367254      0.390223   \n",
       "DBS: 0.75, 3, 4, 3, 5                       0.809161      0.357674   \n",
       "Michaelos: (D,)                             0.000000      0.290860   \n",
       "Spiteful Tit For Tat                        0.000000      0.808891   \n",
       "EugineNier: (D,)                            0.000000      0.539200   \n",
       "Meta Winner: 8 players                      0.295591      0.138113   \n",
       "EvolvedLookerUp1_1_1                        0.000000      0.918104   \n",
       "Forgetful Fool Me Once: 0.05                0.643673      0.100576   \n",
       "NMWE Finite Memory: 86 players              0.265988      0.080901   \n",
       "Stein and Rapoport: 0.05: (D, D)            0.511290      0.565934   \n",
       "NMWE Deterministic: 138 players             0.237255      0.060404   \n",
       "NMWE Long Memory: 114 players               0.342833      0.068507   \n",
       "Evolved ANN 5 Noise 05                      0.859179      0.683188   \n",
       "Nice Meta Winner Ensemble: 200 players      0.326982      0.071031   \n",
       "...                                              ...           ...   \n",
       "ThueMorseInverse                            0.400034      0.586173   \n",
       "Pun1                                        0.390388      1.000000   \n",
       "Cycler CCCCCD                               0.870773      1.000000   \n",
       "Cycler CCD                                  0.648237      1.000000   \n",
       "Defector                                    0.000000      0.000000   \n",
       "Random: 0.3                                 0.306281      0.323607   \n",
       "SelfSteem                                   0.402974      0.608069   \n",
       "Tricky Defector                             1.000000      0.000000   \n",
       "Desperate                                   0.000000      0.000000   \n",
       "Win-Shift Lose-Stay: D                      1.000000      1.000000   \n",
       "Cycler CCCD                                 0.770776      1.000000   \n",
       "Bush Mosteller: 0.5, 0.5, 3.0, 0.5          0.308501      0.363594   \n",
       "Cycler DC                                   0.000000      1.000000   \n",
       "Random: 0.5                                 0.504710      0.513804   \n",
       "Meta Minority: 200 players                  1.000000      0.004208   \n",
       "Bully                                       1.000000      0.000000   \n",
       "Cycler DDC                                  0.000000      0.428263   \n",
       "ThueMorse                                   0.394073      0.688943   \n",
       "Handshake                                   0.917691      0.000000   \n",
       "Negation                                    1.000000      0.000000   \n",
       "Cooperator Hunter                           1.000000      0.000000   \n",
       "SolutionB1                                  1.000000      0.479068   \n",
       "Hopeless                                    1.000000      1.000000   \n",
       "Tricky Cooperator                           1.000000      0.000000   \n",
       "Anti Tit For Tat                            1.000000      0.000000   \n",
       "AntiCycler                                  0.788176      0.849694   \n",
       "$e$                                         1.000000      0.691790   \n",
       "$\\pi$                                       1.000000      0.791274   \n",
       "Gradual Killer: (D, D, D, D, D, C, C)       0.771494      0.201799   \n",
       "$\\phi$                                      1.000000      0.305065   \n",
       "\n",
       "                                        DD_to_C_rate  present  \n",
       "Name                                                           \n",
       "Evolved HMM 5                               0.218408        0  \n",
       "Evolved FSM 16                              0.405417        0  \n",
       "EvolvedLookerUp2_2_2                        0.815119        0  \n",
       "Evolved FSM 16 Noise 05                     0.125542        0  \n",
       "PSO Gambler 2_2_2                           0.422777        0  \n",
       "Evolved ANN                                 0.000000        0  \n",
       "Evolved ANN 5                               0.000000        0  \n",
       "PSO Gambler 1_1_1                           0.163684        0  \n",
       "BackStabber: (D, D)                         0.000000        0  \n",
       "Omega TFT: 3, 8                             0.072430        0  \n",
       "DoubleCrosser: (D, D)                       0.000000        0  \n",
       "Fool Me Once                                0.000000        0  \n",
       "PSO Gambler Mem1                            0.157883        0  \n",
       "Evolved FSM 4                               0.801025        0  \n",
       "Winner12                                    0.562481        0  \n",
       "PSO Gambler 2_2_2 Noise 05                  0.419380        0  \n",
       "MEM2                                        0.000000        0  \n",
       "DBS: 0.75, 3, 4, 3, 5                       0.251448        0  \n",
       "Michaelos: (D,)                             0.000000        0  \n",
       "Spiteful Tit For Tat                        0.000000        0  \n",
       "EugineNier: (D,)                            0.000000        0  \n",
       "Meta Winner: 8 players                      0.067021        0  \n",
       "EvolvedLookerUp1_1_1                        0.000000        0  \n",
       "Forgetful Fool Me Once: 0.05                0.077634        0  \n",
       "NMWE Finite Memory: 86 players              0.141585        0  \n",
       "Stein and Rapoport: 0.05: (D, D)            0.000000        0  \n",
       "NMWE Deterministic: 138 players             0.105043        0  \n",
       "NMWE Long Memory: 114 players               0.106038        0  \n",
       "Evolved ANN 5 Noise 05                      0.275451        0  \n",
       "Nice Meta Winner Ensemble: 200 players      0.126515        0  \n",
       "...                                              ...      ...  \n",
       "ThueMorseInverse                            0.776740        0  \n",
       "Pun1                                        1.000000        0  \n",
       "Cycler CCCCCD                               1.000000        0  \n",
       "Cycler CCD                                  1.000000        0  \n",
       "Defector                                    0.000000        0  \n",
       "Random: 0.3                                 0.312181        0  \n",
       "SelfSteem                                   0.288449        0  \n",
       "Tricky Defector                             0.298177        0  \n",
       "Desperate                                   1.000000        0  \n",
       "Win-Shift Lose-Stay: D                      0.000000        0  \n",
       "Cycler CCCD                                 1.000000        0  \n",
       "Bush Mosteller: 0.5, 0.5, 3.0, 0.5          0.341624        0  \n",
       "Cycler DC                                   1.000000        0  \n",
       "Random: 0.5                                 0.516803        0  \n",
       "Meta Minority: 200 players                  0.998763        0  \n",
       "Bully                                       1.000000        0  \n",
       "Cycler DDC                                  0.622247        0  \n",
       "ThueMorse                                   0.755469        0  \n",
       "Handshake                                   0.711122        0  \n",
       "Negation                                    1.000000        0  \n",
       "Cooperator Hunter                           1.000000        0  \n",
       "SolutionB1                                  0.922667        0  \n",
       "Hopeless                                    1.000000        0  \n",
       "Tricky Cooperator                           1.000000        0  \n",
       "Anti Tit For Tat                            1.000000        0  \n",
       "AntiCycler                                  0.974108        0  \n",
       "$e$                                         1.000000        0  \n",
       "$\\pi$                                       1.000000        0  \n",
       "Gradual Killer: (D, D, D, D, D, C, C)       0.164649        0  \n",
       "$\\phi$                                      1.000000        0  \n",
       "\n",
       "[231 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summing all the csv files and just meaning by subgroup defined by name\n",
    "#SumTable = pd.read_csv('data/tour_1_result_0.csv')\n",
    "list_ = []\n",
    "for file in glob.glob('data/tour*result*.csv'):\n",
    "    table = pd.read_csv(file)\n",
    "    list_.append(table)\n",
    "SumTable = pd.concat(list_, ignore_index=True)\n",
    "AvgTable = SumTable.groupby('Name').mean()\n",
    "\n",
    "\n",
    "AvgTable['present'] = 0  # make new column\n",
    "\n",
    "AvgTable.sort_values(['Rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41224"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(list_)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [2]],\n",
       "\n",
       "       [[3],\n",
       "        [4]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([[1, 2], [3, 4]])[:, :, None]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AvgTable['present'] = 1\n",
    "\n",
    "# create a bunch of training data, each of which only has rows of the participating players\n",
    "# takes a few minutes to run\n",
    "\n",
    "x_train = [pd.DataFrame(0, index=AvgTable.index, columns=AvgTable.columns) for i in range(m)]\n",
    "\n",
    "for i in range(m):\n",
    "    x_train[i].loc[list_[i]['Name']] = AvgTable.loc[list_[i]['Name']]\n",
    "    \n",
    "x_train[3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = np.array([np.array(x) for x in x_train])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras.losses.categorical_crossentropy>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train_top_3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train*1/3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#y_train = np.any(x_train, axis=-1)  # binary for which strategies are present ;; dosen't work with finding top 5\n",
    "\n",
    "y_train = [pd.DataFrame(0, index=AvgTable.index, columns=['Rank']) for i in range(m)]\n",
    "\n",
    "for i in range(m):\n",
    "    y_train[i].loc[list_[i][list_[i]['Rank'] < 3]['Name']] = 1 / 3\n",
    "\n",
    "y_train[3]\n",
    "\n",
    "y_train = np.array([np.array(y).flatten() for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('x_train', x_train)\n",
    "np.save('y_train_top_3', y_train)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41224, 3234), (41224, 231))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41224, 3234)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.reshape(x_train.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Max_8:0' shape=(10000,) dtype=float64>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def top_k_in_k(y_true, y_pred):\n",
    "    #print(y_true, y_pred)\n",
    "    # problem: this doesn't work when passed into K.metrics b/c it doesn't operate on tensors\n",
    "    return K.mean(K.sum(y_true*y_pred, axis=1)/K.max(y_true, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_true[:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('nn_model_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17156177 0.22888893 0.11843825 ... 0.26377254 0.22792148 0.23469573]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [[2.44969560e-05 3.58584075e-05 1.64192115e-05 ... 4.10539942e-05\n",
      "  5.80077995e-06 3.61800107e-06]\n",
      " [1.01834288e-04 2.05747110e-05 1.10211447e-04 ... 4.77498506e-05\n",
      "  7.15845556e-04 5.28877672e-06]\n",
      " [2.40475146e-07 2.35540756e-05 2.15567525e-05 ... 3.12237967e-06\n",
      "  6.78689887e-08 3.47387504e-06]\n",
      " ...\n",
      " [4.29935909e-07 1.86418729e-05 6.72324240e-05 ... 9.38614630e-06\n",
      "  3.41235027e-05 2.70765054e-06]\n",
      " [5.62133173e-05 5.13862215e-05 2.82662113e-05 ... 1.55893897e-04\n",
      "  3.12535485e-05 1.84783305e-04]\n",
      " [5.10971449e-06 1.05882915e-04 4.15164914e-06 ... 5.06928018e-07\n",
      "  3.34200695e-06 1.18863511e-06]]\n",
      "0.7498292260815234\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "y_pred = model.predict(x_train[:n])\n",
    "y_true = y_train[:n]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(K.sum(y_true*y_pred, axis=1).eval())\n",
    "    print(sess.run(top_k_in_k(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0100768 , 0.00429371, 0.0081558 , ..., 0.01153485, 0.0077451 ,\n",
       "       0.00683453])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_true*y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32979 samples, validate on 8245 samples\n",
      "Epoch 1/500\n",
      "32979/32979 [==============================] - 16s 499us/step - loss: 0.0420 - binary_accuracy: 0.9795 - categorical_accuracy: 0.0467 - top_k_in_k: 0.0852 - val_loss: 0.0254 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.0782 - val_top_k_in_k: 0.0588\n",
      "Epoch 2/500\n",
      "32979/32979 [==============================] - 11s 337us/step - loss: 0.0248 - binary_accuracy: 0.9870 - categorical_accuracy: 0.1095 - top_k_in_k: 0.0686 - val_loss: 0.0243 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.1397 - val_top_k_in_k: 0.0734\n",
      "Epoch 3/500\n",
      "32979/32979 [==============================] - 10s 310us/step - loss: 0.0238 - binary_accuracy: 0.9870 - categorical_accuracy: 0.1673 - top_k_in_k: 0.0829 - val_loss: 0.0235 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.1788 - val_top_k_in_k: 0.0902\n",
      "Epoch 4/500\n",
      "32979/32979 [==============================] - 10s 309us/step - loss: 0.0230 - binary_accuracy: 0.9870 - categorical_accuracy: 0.2049 - top_k_in_k: 0.0998 - val_loss: 0.0228 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.2087 - val_top_k_in_k: 0.1049\n",
      "Epoch 5/500\n",
      "32979/32979 [==============================] - 11s 319us/step - loss: 0.0222 - binary_accuracy: 0.9870 - categorical_accuracy: 0.2294 - top_k_in_k: 0.1174 - val_loss: 0.0222 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.2312 - val_top_k_in_k: 0.1229\n",
      "Epoch 6/500\n",
      "32979/32979 [==============================] - 10s 318us/step - loss: 0.0216 - binary_accuracy: 0.9870 - categorical_accuracy: 0.2513 - top_k_in_k: 0.1365 - val_loss: 0.0215 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.2502 - val_top_k_in_k: 0.1399\n",
      "Epoch 7/500\n",
      "32979/32979 [==============================] - 10s 311us/step - loss: 0.0209 - binary_accuracy: 0.9870 - categorical_accuracy: 0.2748 - top_k_in_k: 0.1577 - val_loss: 0.0209 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.2714 - val_top_k_in_k: 0.1598\n",
      "Epoch 8/500\n",
      "32979/32979 [==============================] - 10s 306us/step - loss: 0.0202 - binary_accuracy: 0.9870 - categorical_accuracy: 0.2923 - top_k_in_k: 0.1822 - val_loss: 0.0203 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.2811 - val_top_k_in_k: 0.1854\n",
      "Epoch 9/500\n",
      "32979/32979 [==============================] - 10s 289us/step - loss: 0.0196 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3097 - top_k_in_k: 0.2107 - val_loss: 0.0197 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.2962 - val_top_k_in_k: 0.2098\n",
      "Epoch 10/500\n",
      "32979/32979 [==============================] - 9s 279us/step - loss: 0.0189 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3207 - top_k_in_k: 0.2424 - val_loss: 0.0191 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3118 - val_top_k_in_k: 0.2448\n",
      "Epoch 11/500\n",
      "32979/32979 [==============================] - 9s 280us/step - loss: 0.0183 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3281 - top_k_in_k: 0.2757 - val_loss: 0.0185 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3141 - val_top_k_in_k: 0.2737\n",
      "Epoch 12/500\n",
      "32979/32979 [==============================] - 9s 271us/step - loss: 0.0178 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3341 - top_k_in_k: 0.3094 - val_loss: 0.0180 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3209 - val_top_k_in_k: 0.2980\n",
      "Epoch 13/500\n",
      "32979/32979 [==============================] - 9s 261us/step - loss: 0.0173 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3382 - top_k_in_k: 0.3411 - val_loss: 0.0176 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3216 - val_top_k_in_k: 0.3336\n",
      "Epoch 14/500\n",
      "32979/32979 [==============================] - 8s 256us/step - loss: 0.0169 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3389 - top_k_in_k: 0.3711 - val_loss: 0.0172 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3273 - val_top_k_in_k: 0.3578\n",
      "Epoch 15/500\n",
      "32979/32979 [==============================] - 9s 260us/step - loss: 0.0165 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3397 - top_k_in_k: 0.3978 - val_loss: 0.0169 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3187 - val_top_k_in_k: 0.3895\n",
      "Epoch 16/500\n",
      "32979/32979 [==============================] - 9s 260us/step - loss: 0.0161 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3397 - top_k_in_k: 0.4222 - val_loss: 0.0166 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3213 - val_top_k_in_k: 0.3867\n",
      "Epoch 17/500\n",
      "32979/32979 [==============================] - 9s 275us/step - loss: 0.0158 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3414 - top_k_in_k: 0.4424 - val_loss: 0.0163 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3181 - val_top_k_in_k: 0.4339\n",
      "Epoch 18/500\n",
      "32979/32979 [==============================] - 9s 267us/step - loss: 0.0156 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3413 - top_k_in_k: 0.4622 - val_loss: 0.0161 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3250 - val_top_k_in_k: 0.4389\n",
      "Epoch 19/500\n",
      "32979/32979 [==============================] - 9s 265us/step - loss: 0.0153 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3416 - top_k_in_k: 0.4786 - val_loss: 0.0159 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3187 - val_top_k_in_k: 0.4465\n",
      "Epoch 20/500\n",
      "32979/32979 [==============================] - 8s 257us/step - loss: 0.0151 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3425 - top_k_in_k: 0.4933 - val_loss: 0.0157 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3167 - val_top_k_in_k: 0.4629\n",
      "Epoch 21/500\n",
      "32979/32979 [==============================] - 9s 260us/step - loss: 0.0149 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3438 - top_k_in_k: 0.5065 - val_loss: 0.0155 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3082 - val_top_k_in_k: 0.4753\n",
      "Epoch 22/500\n",
      "32979/32979 [==============================] - 9s 260us/step - loss: 0.0147 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3431 - top_k_in_k: 0.5184 - val_loss: 0.0154 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3077 - val_top_k_in_k: 0.4709\n",
      "Epoch 23/500\n",
      "32979/32979 [==============================] - 8s 257us/step - loss: 0.0145 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3436 - top_k_in_k: 0.5292 - val_loss: 0.0152 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3202 - val_top_k_in_k: 0.5015\n",
      "Epoch 24/500\n",
      "32979/32979 [==============================] - 9s 262us/step - loss: 0.0144 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3430 - top_k_in_k: 0.5390 - val_loss: 0.0151 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3168 - val_top_k_in_k: 0.5180\n",
      "Epoch 25/500\n",
      "32979/32979 [==============================] - 9s 267us/step - loss: 0.0142 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3440 - top_k_in_k: 0.5484 - val_loss: 0.0150 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3210 - val_top_k_in_k: 0.4987\n",
      "Epoch 26/500\n",
      "32979/32979 [==============================] - 8s 257us/step - loss: 0.0141 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3441 - top_k_in_k: 0.5565 - val_loss: 0.0149 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3264 - val_top_k_in_k: 0.5254\n",
      "Epoch 27/500\n",
      "32979/32979 [==============================] - 9s 264us/step - loss: 0.0140 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3450 - top_k_in_k: 0.5639 - val_loss: 0.0147 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3235 - val_top_k_in_k: 0.5258\n",
      "Epoch 28/500\n",
      "32979/32979 [==============================] - 9s 258us/step - loss: 0.0139 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3476 - top_k_in_k: 0.5711 - val_loss: 0.0147 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3189 - val_top_k_in_k: 0.5470\n",
      "Epoch 29/500\n",
      "32979/32979 [==============================] - 9s 258us/step - loss: 0.0137 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3474 - top_k_in_k: 0.5775 - val_loss: 0.0146 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3144 - val_top_k_in_k: 0.5434\n",
      "Epoch 30/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32979/32979 [==============================] - 9s 260us/step - loss: 0.0136 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3468 - top_k_in_k: 0.5841 - val_loss: 0.0145 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3300 - val_top_k_in_k: 0.5476\n",
      "Epoch 31/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0135 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3493 - top_k_in_k: 0.5899 - val_loss: 0.0144 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3303 - val_top_k_in_k: 0.5441\n",
      "Epoch 32/500\n",
      "32979/32979 [==============================] - 9s 258us/step - loss: 0.0134 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3497 - top_k_in_k: 0.5951 - val_loss: 0.0143 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3157 - val_top_k_in_k: 0.5440\n",
      "Epoch 33/500\n",
      "32979/32979 [==============================] - 9s 258us/step - loss: 0.0133 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3503 - top_k_in_k: 0.6001 - val_loss: 0.0142 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3281 - val_top_k_in_k: 0.5603\n",
      "Epoch 34/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0133 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3515 - top_k_in_k: 0.6054 - val_loss: 0.0142 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3271 - val_top_k_in_k: 0.5632\n",
      "Epoch 35/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0132 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3511 - top_k_in_k: 0.6099 - val_loss: 0.0141 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3216 - val_top_k_in_k: 0.5622\n",
      "Epoch 36/500\n",
      "32979/32979 [==============================] - 8s 232us/step - loss: 0.0131 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3525 - top_k_in_k: 0.6137 - val_loss: 0.0141 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3237 - val_top_k_in_k: 0.5741\n",
      "Epoch 37/500\n",
      "32979/32979 [==============================] - 8s 254us/step - loss: 0.0130 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3518 - top_k_in_k: 0.6182 - val_loss: 0.0140 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3317 - val_top_k_in_k: 0.5811\n",
      "Epoch 38/500\n",
      "32979/32979 [==============================] - 8s 249us/step - loss: 0.0129 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3525 - top_k_in_k: 0.6224 - val_loss: 0.0139 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3339 - val_top_k_in_k: 0.5703\n",
      "Epoch 39/500\n",
      "32979/32979 [==============================] - 8s 253us/step - loss: 0.0129 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3526 - top_k_in_k: 0.6262 - val_loss: 0.0139 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3233 - val_top_k_in_k: 0.5645\n",
      "Epoch 40/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0128 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3535 - top_k_in_k: 0.6293 - val_loss: 0.0138 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3332 - val_top_k_in_k: 0.5929\n",
      "Epoch 41/500\n",
      "32979/32979 [==============================] - 8s 247us/step - loss: 0.0128 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3540 - top_k_in_k: 0.6332 - val_loss: 0.0138 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3224 - val_top_k_in_k: 0.5831\n",
      "Epoch 42/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0127 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3524 - top_k_in_k: 0.6366 - val_loss: 0.0138 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3264 - val_top_k_in_k: 0.5828\n",
      "Epoch 43/500\n",
      "32979/32979 [==============================] - 8s 254us/step - loss: 0.0126 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3538 - top_k_in_k: 0.6395 - val_loss: 0.0137 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3221 - val_top_k_in_k: 0.6063\n",
      "Epoch 44/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0126 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3538 - top_k_in_k: 0.6428 - val_loss: 0.0137 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3219 - val_top_k_in_k: 0.5919\n",
      "Epoch 45/500\n",
      "32979/32979 [==============================] - 8s 234us/step - loss: 0.0125 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3539 - top_k_in_k: 0.6459 - val_loss: 0.0136 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3392 - val_top_k_in_k: 0.5864\n",
      "Epoch 46/500\n",
      "32979/32979 [==============================] - 8s 254us/step - loss: 0.0125 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3535 - top_k_in_k: 0.6485 - val_loss: 0.0136 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3332 - val_top_k_in_k: 0.5979\n",
      "Epoch 47/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0124 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3549 - top_k_in_k: 0.6511 - val_loss: 0.0136 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3344 - val_top_k_in_k: 0.6164\n",
      "Epoch 48/500\n",
      "32979/32979 [==============================] - 8s 251us/step - loss: 0.0124 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3544 - top_k_in_k: 0.6540 - val_loss: 0.0135 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3305 - val_top_k_in_k: 0.6091\n",
      "Epoch 49/500\n",
      "32979/32979 [==============================] - 9s 262us/step - loss: 0.0123 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3542 - top_k_in_k: 0.6569 - val_loss: 0.0135 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3322 - val_top_k_in_k: 0.6019\n",
      "Epoch 50/500\n",
      "32979/32979 [==============================] - 8s 254us/step - loss: 0.0123 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3552 - top_k_in_k: 0.6593 - val_loss: 0.0135 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3310 - val_top_k_in_k: 0.5973\n",
      "Epoch 51/500\n",
      "32979/32979 [==============================] - 8s 254us/step - loss: 0.0122 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3549 - top_k_in_k: 0.6610 - val_loss: 0.0134 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3254 - val_top_k_in_k: 0.6146\n",
      "Epoch 52/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0122 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3556 - top_k_in_k: 0.6643 - val_loss: 0.0134 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3185 - val_top_k_in_k: 0.6178\n",
      "Epoch 53/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0122 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3559 - top_k_in_k: 0.6660 - val_loss: 0.0134 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3265 - val_top_k_in_k: 0.6218\n",
      "Epoch 54/500\n",
      "32979/32979 [==============================] - 8s 254us/step - loss: 0.0121 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3560 - top_k_in_k: 0.6684 - val_loss: 0.0134 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3162 - val_top_k_in_k: 0.6068\n",
      "Epoch 55/500\n",
      "32979/32979 [==============================] - 8s 255us/step - loss: 0.0121 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3563 - top_k_in_k: 0.6706 - val_loss: 0.0133 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3151 - val_top_k_in_k: 0.6176\n",
      "Epoch 56/500\n",
      "32979/32979 [==============================] - 8s 245us/step - loss: 0.0121 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3563 - top_k_in_k: 0.6728 - val_loss: 0.0133 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3271 - val_top_k_in_k: 0.6081\n",
      "Epoch 57/500\n",
      "32979/32979 [==============================] - 8s 252us/step - loss: 0.0120 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3570 - top_k_in_k: 0.6749 - val_loss: 0.0133 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3367 - val_top_k_in_k: 0.6176\n",
      "Epoch 58/500\n",
      "32979/32979 [==============================] - 8s 244us/step - loss: 0.0120 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3545 - top_k_in_k: 0.6765 - val_loss: 0.0133 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3275 - val_top_k_in_k: 0.6458\n",
      "Epoch 59/500\n",
      "32979/32979 [==============================] - 9s 258us/step - loss: 0.0120 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3580 - top_k_in_k: 0.6786 - val_loss: 0.0133 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3347 - val_top_k_in_k: 0.6298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/500\n",
      "32979/32979 [==============================] - 8s 250us/step - loss: 0.0119 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3566 - top_k_in_k: 0.6806 - val_loss: 0.0132 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3288 - val_top_k_in_k: 0.6173\n",
      "Epoch 61/500\n",
      "32979/32979 [==============================] - 8s 247us/step - loss: 0.0119 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3589 - top_k_in_k: 0.6822 - val_loss: 0.0132 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3236 - val_top_k_in_k: 0.6368\n",
      "Epoch 62/500\n",
      "32979/32979 [==============================] - 9s 259us/step - loss: 0.0119 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3571 - top_k_in_k: 0.6842 - val_loss: 0.0132 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3283 - val_top_k_in_k: 0.6442\n",
      "Epoch 63/500\n",
      "32979/32979 [==============================] - 8s 256us/step - loss: 0.0118 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3577 - top_k_in_k: 0.6858 - val_loss: 0.0132 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3273 - val_top_k_in_k: 0.6277\n",
      "Epoch 64/500\n",
      "32979/32979 [==============================] - 10s 310us/step - loss: 0.0118 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3546 - top_k_in_k: 0.6874 - val_loss: 0.0132 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3322 - val_top_k_in_k: 0.6498\n",
      "Epoch 65/500\n",
      "32979/32979 [==============================] - 10s 301us/step - loss: 0.0118 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3568 - top_k_in_k: 0.6893 - val_loss: 0.0131 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3233 - val_top_k_in_k: 0.6221\n",
      "Epoch 66/500\n",
      "32979/32979 [==============================] - 9s 266us/step - loss: 0.0117 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3573 - top_k_in_k: 0.6908 - val_loss: 0.0131 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3215 - val_top_k_in_k: 0.6182\n",
      "Epoch 67/500\n",
      "32979/32979 [==============================] - 9s 263us/step - loss: 0.0117 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3562 - top_k_in_k: 0.6925 - val_loss: 0.0131 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3271 - val_top_k_in_k: 0.6457\n",
      "Epoch 68/500\n",
      "32979/32979 [==============================] - 10s 290us/step - loss: 0.0117 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3574 - top_k_in_k: 0.6938 - val_loss: 0.0131 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3292 - val_top_k_in_k: 0.6361\n",
      "Epoch 69/500\n",
      "32979/32979 [==============================] - 9s 287us/step - loss: 0.0117 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3571 - top_k_in_k: 0.6954 - val_loss: 0.0131 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3252 - val_top_k_in_k: 0.6241\n",
      "Epoch 70/500\n",
      "32979/32979 [==============================] - 9s 284us/step - loss: 0.0116 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3559 - top_k_in_k: 0.6966 - val_loss: 0.0131 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3187 - val_top_k_in_k: 0.6355\n",
      "Epoch 71/500\n",
      "32979/32979 [==============================] - 9s 276us/step - loss: 0.0116 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3569 - top_k_in_k: 0.6986 - val_loss: 0.0131 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3323 - val_top_k_in_k: 0.6508\n",
      "Epoch 72/500\n",
      "32979/32979 [==============================] - 9s 270us/step - loss: 0.0116 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3572 - top_k_in_k: 0.6994 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3231 - val_top_k_in_k: 0.6265\n",
      "Epoch 73/500\n",
      "32979/32979 [==============================] - 9s 287us/step - loss: 0.0116 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3562 - top_k_in_k: 0.7015 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3107 - val_top_k_in_k: 0.6323\n",
      "Epoch 74/500\n",
      "32979/32979 [==============================] - 10s 300us/step - loss: 0.0116 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3570 - top_k_in_k: 0.7023 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3272 - val_top_k_in_k: 0.6539\n",
      "Epoch 75/500\n",
      "32979/32979 [==============================] - 9s 287us/step - loss: 0.0115 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3567 - top_k_in_k: 0.7039 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3289 - val_top_k_in_k: 0.6325\n",
      "Epoch 76/500\n",
      "32979/32979 [==============================] - 9s 271us/step - loss: 0.0115 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3557 - top_k_in_k: 0.7053 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3263 - val_top_k_in_k: 0.6454\n",
      "Epoch 77/500\n",
      "32979/32979 [==============================] - 9s 276us/step - loss: 0.0115 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3570 - top_k_in_k: 0.7058 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3277 - val_top_k_in_k: 0.6539\n",
      "Epoch 78/500\n",
      "32979/32979 [==============================] - 8s 256us/step - loss: 0.0115 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3551 - top_k_in_k: 0.7075 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3426 - val_top_k_in_k: 0.6484\n",
      "Epoch 79/500\n",
      "32979/32979 [==============================] - 9s 274us/step - loss: 0.0115 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3548 - top_k_in_k: 0.7084 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3231 - val_top_k_in_k: 0.6598\n",
      "Epoch 80/500\n",
      "32979/32979 [==============================] - 9s 271us/step - loss: 0.0114 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3571 - top_k_in_k: 0.7103 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3136 - val_top_k_in_k: 0.6246\n",
      "Epoch 81/500\n",
      "32979/32979 [==============================] - 9s 267us/step - loss: 0.0114 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3556 - top_k_in_k: 0.7110 - val_loss: 0.0130 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3178 - val_top_k_in_k: 0.6591\n",
      "Epoch 82/500\n",
      "32979/32979 [==============================] - 9s 269us/step - loss: 0.0114 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3559 - top_k_in_k: 0.7126 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3209 - val_top_k_in_k: 0.6483\n",
      "Epoch 83/500\n",
      "32979/32979 [==============================] - 9s 272us/step - loss: 0.0114 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3562 - top_k_in_k: 0.7135 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3277 - val_top_k_in_k: 0.6546\n",
      "Epoch 84/500\n",
      "32979/32979 [==============================] - 9s 274us/step - loss: 0.0114 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3554 - top_k_in_k: 0.7144 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3322 - val_top_k_in_k: 0.6486\n",
      "Epoch 85/500\n",
      "32979/32979 [==============================] - 9s 262us/step - loss: 0.0113 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3563 - top_k_in_k: 0.7157 - val_loss: 0.0129 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3254 - val_top_k_in_k: 0.6900\n",
      "Epoch 86/500\n",
      "32979/32979 [==============================] - 10s 290us/step - loss: 0.0113 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3567 - top_k_in_k: 0.7169 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3260 - val_top_k_in_k: 0.6431\n",
      "Epoch 87/500\n",
      "32979/32979 [==============================] - 9s 286us/step - loss: 0.0113 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3570 - top_k_in_k: 0.7179 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3311 - val_top_k_in_k: 0.6436\n",
      "Epoch 88/500\n",
      "32979/32979 [==============================] - 9s 286us/step - loss: 0.0113 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3569 - top_k_in_k: 0.7189 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3431 - val_top_k_in_k: 0.6692\n",
      "Epoch 89/500\n",
      "32979/32979 [==============================] - 9s 287us/step - loss: 0.0113 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3576 - top_k_in_k: 0.7202 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3323 - val_top_k_in_k: 0.6331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/500\n",
      "32979/32979 [==============================] - 10s 296us/step - loss: 0.0113 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3573 - top_k_in_k: 0.7209 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3121 - val_top_k_in_k: 0.6576\n",
      "Epoch 91/500\n",
      "32979/32979 [==============================] - 9s 283us/step - loss: 0.0112 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3555 - top_k_in_k: 0.7219 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3243 - val_top_k_in_k: 0.6441\n",
      "Epoch 92/500\n",
      "32979/32979 [==============================] - 10s 297us/step - loss: 0.0112 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3562 - top_k_in_k: 0.7227 - val_loss: 0.0129 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3269 - val_top_k_in_k: 0.6689\n",
      "Epoch 93/500\n",
      "32979/32979 [==============================] - 9s 283us/step - loss: 0.0112 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3573 - top_k_in_k: 0.7241 - val_loss: 0.0129 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3293 - val_top_k_in_k: 0.6560\n",
      "Epoch 94/500\n",
      "32979/32979 [==============================] - 9s 283us/step - loss: 0.0112 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3563 - top_k_in_k: 0.7247 - val_loss: 0.0128 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3350 - val_top_k_in_k: 0.6765\n",
      "Epoch 95/500\n",
      "32979/32979 [==============================] - 9s 284us/step - loss: 0.0112 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3569 - top_k_in_k: 0.7261 - val_loss: 0.0128 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3191 - val_top_k_in_k: 0.6602\n",
      "Epoch 96/500\n",
      "32979/32979 [==============================] - 9s 276us/step - loss: 0.0112 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3581 - top_k_in_k: 0.7268 - val_loss: 0.0128 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3314 - val_top_k_in_k: 0.6593\n",
      "Epoch 97/500\n",
      "32979/32979 [==============================] - 9s 270us/step - loss: 0.0112 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3557 - top_k_in_k: 0.7274 - val_loss: 0.0128 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3195 - val_top_k_in_k: 0.6981\n",
      "Epoch 98/500\n",
      "32979/32979 [==============================] - 10s 300us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3559 - top_k_in_k: 0.7286 - val_loss: 0.0128 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3347 - val_top_k_in_k: 0.6733\n",
      "Epoch 99/500\n",
      "32979/32979 [==============================] - 9s 282us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3568 - top_k_in_k: 0.7295 - val_loss: 0.0128 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3257 - val_top_k_in_k: 0.6679\n",
      "Epoch 100/500\n",
      "32979/32979 [==============================] - 9s 272us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3577 - top_k_in_k: 0.7303 - val_loss: 0.0128 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3133 - val_top_k_in_k: 0.6536\n",
      "Epoch 101/500\n",
      "32979/32979 [==============================] - 9s 273us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3546 - top_k_in_k: 0.7313 - val_loss: 0.0128 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3289 - val_top_k_in_k: 0.6512\n",
      "Epoch 102/500\n",
      "32979/32979 [==============================] - 10s 303us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3568 - top_k_in_k: 0.7318 - val_loss: 0.0128 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3290 - val_top_k_in_k: 0.6583\n",
      "Epoch 103/500\n",
      "32979/32979 [==============================] - 9s 285us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3573 - top_k_in_k: 0.7330 - val_loss: 0.0128 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3173 - val_top_k_in_k: 0.6606\n",
      "Epoch 104/500\n",
      "32979/32979 [==============================] - 10s 303us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3566 - top_k_in_k: 0.7337 - val_loss: 0.0128 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3206 - val_top_k_in_k: 0.6710\n",
      "Epoch 105/500\n",
      "32979/32979 [==============================] - 10s 296us/step - loss: 0.0110 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3590 - top_k_in_k: 0.7345 - val_loss: 0.0128 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3249 - val_top_k_in_k: 0.6701\n",
      "Epoch 106/500\n",
      "32979/32979 [==============================] - 10s 290us/step - loss: 0.0110 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3560 - top_k_in_k: 0.7356 - val_loss: 0.0128 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3223 - val_top_k_in_k: 0.6794\n"
     ]
    }
   ],
   "source": [
    "\"\"\" sigmoid, binary cross entropy, 0.03 LR\n",
    "\n",
    "Epoch 106/500\n",
    "32979/32979 [==============================] - 10s 290us/step - loss: 0.0110 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3560 - top_k_in_k: 0.7356 - val_loss: 0.0128 - val_binary_accuracy: 0.9869 - val_categorical_accuracy: 0.3223 - val_top_k_in_k: 0.6794\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense(units=400, activation='relu', input_dim=3234))\n",
    "model3.add(Dense(units=300, activation='relu'))\n",
    "model3.add(Dense(units=231, activation='sigmoid'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=0.03, decay=1e-6, momentum=0.9),\n",
    "              metrics=['binary_accuracy', 'categorical_accuracy', top_k_in_k])\n",
    "\n",
    "history3 = model3.fit(np.array(x_train), np.array(y_train), \n",
    "                    validation_split=0.2, \n",
    "                    epochs=500, \n",
    "                    verbose=1, \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')],\n",
    "                    batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32979 samples, validate on 8245 samples\n",
      "Epoch 51/500\n",
      "32979/32979 [==============================] - 9s 284us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3520 - top_k_in_k: 0.7102 - val_loss: 0.0124 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3315 - val_top_k_in_k: 0.6642\n",
      "Epoch 52/500\n",
      "32979/32979 [==============================] - 9s 280us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3540 - top_k_in_k: 0.7107 - val_loss: 0.0124 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3324 - val_top_k_in_k: 0.6656\n",
      "Epoch 53/500\n",
      "32979/32979 [==============================] - 10s 292us/step - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3521 - top_k_in_k: 0.7111 - val_loss: 0.0124 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3253 - val_top_k_in_k: 0.6677\n",
      "Epoch 54/500\n",
      "20400/32979 [=================>............] - ETA: 3s - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3493 - top_k_in_k: 0.7124"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-ed1494d1ea42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     batch_size=50)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2475\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## keras.models.load_model('nn_model_v1.h5')\n",
    "\n",
    "\"\"\" softmax, binary, LR 0.01\n",
    "\n",
    "Epoch 204/500\n",
    "20400/32979 [=================>............] - ETA: 3s - loss: 0.0111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3493 - top_k_in_k: 0.7124\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#basic sequenctial keras NN model with 2 layers.\n",
    "#Input are vectors of strategies that participate in the tournament\n",
    "#Output is prediction of 1 hot vector representing the 3 most dominant strategies in the tournament\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Dense(units=400, activation='relu', input_dim=3234))\n",
    "# model.add(Dense(units=300, activation='relu'))\n",
    "# model.add(Dense(units=231, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9),\n",
    "#               metrics=['binary_accuracy', 'categorical_accuracy', top_k_in_k])\n",
    "\n",
    "history = model.fit(np.array(x_train), np.array(y_train), \n",
    "                    validation_split=0.2, \n",
    "                    initial_epoch=200,\n",
    "                    epochs=500, \n",
    "                    verbose=1, \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')],\n",
    "                    batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nn_model_softmax_binary.h5')\n",
    "model2.save('nn_model_logis.h5')\n",
    "model3.save('nn_model_sigmoid_binary.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_binary_accuracy', 'val_categorical_accuracy', 'loss', 'binary_accuracy', 'categorical_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiKiEUkwERHRiP8CN2Vost9PeFUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1250856d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125215c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125896da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32979 samples, validate on 8245 samples\n",
      "Epoch 1/5000\n",
      "32979/32979 [==============================] - 10s 303us/step - loss: 5.6985 - binary_accuracy: 0.9870 - categorical_accuracy: 0.0491 - top_k_in_k: 0.0527 - val_loss: 4.9151 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.1008 - val_top_k_in_k: 0.0902\n",
      "Epoch 2/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 4.4942 - binary_accuracy: 0.9870 - categorical_accuracy: 0.1481 - top_k_in_k: 0.1204 - val_loss: 4.1725 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.1910 - val_top_k_in_k: 0.1451\n",
      "Epoch 3/5000\n",
      "32979/32979 [==============================] - 6s 167us/step - loss: 3.8893 - binary_accuracy: 0.9870 - categorical_accuracy: 0.2342 - top_k_in_k: 0.1745 - val_loss: 3.7120 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.2615 - val_top_k_in_k: 0.1958\n",
      "Epoch 4/5000\n",
      "32979/32979 [==============================] - 6s 182us/step - loss: 3.4920 - binary_accuracy: 0.9870 - categorical_accuracy: 0.2878 - top_k_in_k: 0.2253 - val_loss: 3.3933 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3003 - val_top_k_in_k: 0.2429\n",
      "Epoch 5/5000\n",
      "32979/32979 [==============================] - 6s 196us/step - loss: 3.2078 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3177 - top_k_in_k: 0.2719 - val_loss: 3.1577 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3113 - val_top_k_in_k: 0.2849\n",
      "Epoch 6/5000\n",
      "32979/32979 [==============================] - 7s 199us/step - loss: 2.9938 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3308 - top_k_in_k: 0.3136 - val_loss: 2.9766 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3241 - val_top_k_in_k: 0.3220\n",
      "Epoch 7/5000\n",
      "32979/32979 [==============================] - 6s 192us/step - loss: 2.8270 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3383 - top_k_in_k: 0.3496 - val_loss: 2.8335 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3320 - val_top_k_in_k: 0.3550\n",
      "Epoch 8/5000\n",
      "32979/32979 [==============================] - 6s 190us/step - loss: 2.6933 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3446 - top_k_in_k: 0.3816 - val_loss: 2.7180 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3354 - val_top_k_in_k: 0.3822\n",
      "Epoch 9/5000\n",
      "32979/32979 [==============================] - 5s 166us/step - loss: 2.5839 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3488 - top_k_in_k: 0.4085 - val_loss: 2.6219 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3380 - val_top_k_in_k: 0.4077\n",
      "Epoch 10/5000\n",
      "32979/32979 [==============================] - 5s 162us/step - loss: 2.4927 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3535 - top_k_in_k: 0.4322 - val_loss: 2.5420 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3425 - val_top_k_in_k: 0.4284\n",
      "Epoch 11/5000\n",
      "32979/32979 [==============================] - 5s 159us/step - loss: 2.4157 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3561 - top_k_in_k: 0.4531 - val_loss: 2.4739 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3446 - val_top_k_in_k: 0.4474\n",
      "Epoch 12/5000\n",
      "32979/32979 [==============================] - 6s 168us/step - loss: 2.3498 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3573 - top_k_in_k: 0.4711 - val_loss: 2.4151 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3459 - val_top_k_in_k: 0.4648\n",
      "Epoch 13/5000\n",
      "32979/32979 [==============================] - 5s 166us/step - loss: 2.2928 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3603 - top_k_in_k: 0.4872 - val_loss: 2.3645 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3505 - val_top_k_in_k: 0.4807\n",
      "Epoch 14/5000\n",
      "32979/32979 [==============================] - 6s 172us/step - loss: 2.2430 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3607 - top_k_in_k: 0.5020 - val_loss: 2.3202 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3475 - val_top_k_in_k: 0.4931\n",
      "Epoch 15/5000\n",
      "32979/32979 [==============================] - 5s 164us/step - loss: 2.1992 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3615 - top_k_in_k: 0.5150 - val_loss: 2.2813 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3506 - val_top_k_in_k: 0.5045\n",
      "Epoch 16/5000\n",
      "32979/32979 [==============================] - 6s 168us/step - loss: 2.1603 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3619 - top_k_in_k: 0.5267 - val_loss: 2.2468 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3510 - val_top_k_in_k: 0.5149\n",
      "Epoch 17/5000\n",
      "32979/32979 [==============================] - 5s 160us/step - loss: 2.1256 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3621 - top_k_in_k: 0.5371 - val_loss: 2.2155 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3532 - val_top_k_in_k: 0.5250\n",
      "Epoch 18/5000\n",
      "32979/32979 [==============================] - 5s 160us/step - loss: 2.0944 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3637 - top_k_in_k: 0.5470 - val_loss: 2.1879 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3526 - val_top_k_in_k: 0.5331\n",
      "Epoch 19/5000\n",
      "32979/32979 [==============================] - 5s 162us/step - loss: 2.0663 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3637 - top_k_in_k: 0.5557 - val_loss: 2.1627 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3546 - val_top_k_in_k: 0.5424\n",
      "Epoch 20/5000\n",
      "32979/32979 [==============================] - 5s 159us/step - loss: 2.0408 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3652 - top_k_in_k: 0.5640 - val_loss: 2.1400 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3497 - val_top_k_in_k: 0.5489\n",
      "Epoch 21/5000\n",
      "32979/32979 [==============================] - 6s 167us/step - loss: 2.0176 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3653 - top_k_in_k: 0.5710 - val_loss: 2.1191 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3580 - val_top_k_in_k: 0.5570\n",
      "Epoch 22/5000\n",
      "32979/32979 [==============================] - 6s 180us/step - loss: 1.9963 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3671 - top_k_in_k: 0.5782 - val_loss: 2.1003 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3523 - val_top_k_in_k: 0.5633\n",
      "Epoch 23/5000\n",
      "32979/32979 [==============================] - 6s 174us/step - loss: 1.9768 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3650 - top_k_in_k: 0.5848 - val_loss: 2.0831 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3571 - val_top_k_in_k: 0.5692\n",
      "Epoch 24/5000\n",
      "32979/32979 [==============================] - 6s 175us/step - loss: 1.9588 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3672 - top_k_in_k: 0.5909 - val_loss: 2.0670 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3523 - val_top_k_in_k: 0.5741\n",
      "Epoch 25/5000\n",
      "32979/32979 [==============================] - 6s 174us/step - loss: 1.9423 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3664 - top_k_in_k: 0.5962 - val_loss: 2.0524 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3555 - val_top_k_in_k: 0.5798\n",
      "Epoch 26/5000\n",
      "32979/32979 [==============================] - 6s 170us/step - loss: 1.9269 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3671 - top_k_in_k: 0.6017 - val_loss: 2.0387 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3582 - val_top_k_in_k: 0.5838\n",
      "Epoch 27/5000\n",
      "32979/32979 [==============================] - 6s 176us/step - loss: 1.9126 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3658 - top_k_in_k: 0.6062 - val_loss: 2.0263 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3545 - val_top_k_in_k: 0.5890\n",
      "Epoch 28/5000\n",
      "32979/32979 [==============================] - 6s 170us/step - loss: 1.8993 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3668 - top_k_in_k: 0.6112 - val_loss: 2.0144 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3482 - val_top_k_in_k: 0.5926\n",
      "Epoch 29/5000\n",
      "32979/32979 [==============================] - 6s 176us/step - loss: 1.8868 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3660 - top_k_in_k: 0.6153 - val_loss: 2.0036 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3571 - val_top_k_in_k: 0.5972\n",
      "Epoch 30/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32979/32979 [==============================] - 6s 174us/step - loss: 1.8752 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3654 - top_k_in_k: 0.6196 - val_loss: 1.9934 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3569 - val_top_k_in_k: 0.6004\n",
      "Epoch 31/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.8642 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3664 - top_k_in_k: 0.6233 - val_loss: 1.9841 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3569 - val_top_k_in_k: 0.6051\n",
      "Epoch 32/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.8540 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3668 - top_k_in_k: 0.6272 - val_loss: 1.9748 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3540 - val_top_k_in_k: 0.6079\n",
      "Epoch 33/5000\n",
      "32979/32979 [==============================] - 5s 158us/step - loss: 1.8444 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3660 - top_k_in_k: 0.6306 - val_loss: 1.9667 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3534 - val_top_k_in_k: 0.6110\n",
      "Epoch 34/5000\n",
      "32979/32979 [==============================] - 5s 159us/step - loss: 1.8353 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3681 - top_k_in_k: 0.6341 - val_loss: 1.9589 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3542 - val_top_k_in_k: 0.6131\n",
      "Epoch 35/5000\n",
      "32979/32979 [==============================] - 5s 165us/step - loss: 1.8267 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3675 - top_k_in_k: 0.6367 - val_loss: 1.9515 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3560 - val_top_k_in_k: 0.6166\n",
      "Epoch 36/5000\n",
      "32979/32979 [==============================] - 5s 163us/step - loss: 1.8186 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3675 - top_k_in_k: 0.6401 - val_loss: 1.9444 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3540 - val_top_k_in_k: 0.6197\n",
      "Epoch 37/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.8109 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3665 - top_k_in_k: 0.6430 - val_loss: 1.9376 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3540 - val_top_k_in_k: 0.6218\n",
      "Epoch 38/5000\n",
      "32979/32979 [==============================] - 5s 163us/step - loss: 1.8036 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3650 - top_k_in_k: 0.6454 - val_loss: 1.9316 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3549 - val_top_k_in_k: 0.6244\n",
      "Epoch 39/5000\n",
      "32979/32979 [==============================] - 6s 171us/step - loss: 1.7966 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3650 - top_k_in_k: 0.6482 - val_loss: 1.9256 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3571 - val_top_k_in_k: 0.6271\n",
      "Epoch 40/5000\n",
      "32979/32979 [==============================] - 5s 166us/step - loss: 1.7900 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3663 - top_k_in_k: 0.6508 - val_loss: 1.9202 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3522 - val_top_k_in_k: 0.6284\n",
      "Epoch 41/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.7837 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3658 - top_k_in_k: 0.6529 - val_loss: 1.9147 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3517 - val_top_k_in_k: 0.6314\n",
      "Epoch 42/5000\n",
      "32979/32979 [==============================] - 6s 168us/step - loss: 1.7777 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3663 - top_k_in_k: 0.6554 - val_loss: 1.9099 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3506 - val_top_k_in_k: 0.6335\n",
      "Epoch 43/5000\n",
      "32979/32979 [==============================] - 5s 156us/step - loss: 1.7719 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3653 - top_k_in_k: 0.6577 - val_loss: 1.9049 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3532 - val_top_k_in_k: 0.6352\n",
      "Epoch 44/5000\n",
      "32979/32979 [==============================] - 5s 167us/step - loss: 1.7664 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3663 - top_k_in_k: 0.6597 - val_loss: 1.9002 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3508 - val_top_k_in_k: 0.6374\n",
      "Epoch 45/5000\n",
      "32979/32979 [==============================] - 6s 177us/step - loss: 1.7612 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3655 - top_k_in_k: 0.6619 - val_loss: 1.8961 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3497 - val_top_k_in_k: 0.6385\n",
      "Epoch 46/5000\n",
      "32979/32979 [==============================] - 6s 173us/step - loss: 1.7562 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3645 - top_k_in_k: 0.6636 - val_loss: 1.8917 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3526 - val_top_k_in_k: 0.6406\n",
      "Epoch 47/5000\n",
      "32979/32979 [==============================] - 5s 161us/step - loss: 1.7513 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3638 - top_k_in_k: 0.6656 - val_loss: 1.8881 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3508 - val_top_k_in_k: 0.6418\n",
      "Epoch 48/5000\n",
      "32979/32979 [==============================] - 6s 194us/step - loss: 1.7467 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3644 - top_k_in_k: 0.6672 - val_loss: 1.8841 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3493 - val_top_k_in_k: 0.6438\n",
      "Epoch 49/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.7423 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3653 - top_k_in_k: 0.6692 - val_loss: 1.8805 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3526 - val_top_k_in_k: 0.6456\n",
      "Epoch 50/5000\n",
      "32979/32979 [==============================] - 6s 168us/step - loss: 1.7379 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3656 - top_k_in_k: 0.6707 - val_loss: 1.8773 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3531 - val_top_k_in_k: 0.6475\n",
      "Epoch 51/5000\n",
      "32979/32979 [==============================] - 6s 167us/step - loss: 1.7338 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3654 - top_k_in_k: 0.6724 - val_loss: 1.8740 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3475 - val_top_k_in_k: 0.6492\n",
      "Epoch 52/5000\n",
      "32979/32979 [==============================] - 5s 158us/step - loss: 1.7299 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3625 - top_k_in_k: 0.6740 - val_loss: 1.8708 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3533 - val_top_k_in_k: 0.6502\n",
      "Epoch 53/5000\n",
      "32979/32979 [==============================] - 5s 160us/step - loss: 1.7261 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3644 - top_k_in_k: 0.6757 - val_loss: 1.8676 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3520 - val_top_k_in_k: 0.6512\n",
      "Epoch 54/5000\n",
      "32979/32979 [==============================] - 5s 154us/step - loss: 1.7224 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3633 - top_k_in_k: 0.6770 - val_loss: 1.8647 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3517 - val_top_k_in_k: 0.6522\n",
      "Epoch 55/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.7188 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3647 - top_k_in_k: 0.6783 - val_loss: 1.8620 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3516 - val_top_k_in_k: 0.6543\n",
      "Epoch 56/5000\n",
      "32979/32979 [==============================] - 6s 189us/step - loss: 1.7154 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3646 - top_k_in_k: 0.6799 - val_loss: 1.8593 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3497 - val_top_k_in_k: 0.6553\n",
      "Epoch 57/5000\n",
      "32979/32979 [==============================] - 6s 190us/step - loss: 1.7121 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3637 - top_k_in_k: 0.6810 - val_loss: 1.8566 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3515 - val_top_k_in_k: 0.6570\n",
      "Epoch 58/5000\n",
      "32979/32979 [==============================] - 6s 190us/step - loss: 1.7089 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3640 - top_k_in_k: 0.6825 - val_loss: 1.8542 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3492 - val_top_k_in_k: 0.6580\n",
      "Epoch 59/5000\n",
      "32979/32979 [==============================] - 6s 184us/step - loss: 1.7058 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3639 - top_k_in_k: 0.6837 - val_loss: 1.8518 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3488 - val_top_k_in_k: 0.6586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/5000\n",
      "32979/32979 [==============================] - 6s 183us/step - loss: 1.7027 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3635 - top_k_in_k: 0.6848 - val_loss: 1.8495 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3517 - val_top_k_in_k: 0.6604\n",
      "Epoch 61/5000\n",
      "32979/32979 [==============================] - 6s 197us/step - loss: 1.6999 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3637 - top_k_in_k: 0.6862 - val_loss: 1.8473 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3525 - val_top_k_in_k: 0.6609\n",
      "Epoch 62/5000\n",
      "32979/32979 [==============================] - 6s 188us/step - loss: 1.6970 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3637 - top_k_in_k: 0.6873 - val_loss: 1.8451 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3527 - val_top_k_in_k: 0.6619\n",
      "Epoch 63/5000\n",
      "32979/32979 [==============================] - 6s 192us/step - loss: 1.6943 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3648 - top_k_in_k: 0.6882 - val_loss: 1.8428 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3503 - val_top_k_in_k: 0.6630\n",
      "Epoch 64/5000\n",
      "32979/32979 [==============================] - 6s 186us/step - loss: 1.6916 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3645 - top_k_in_k: 0.6894 - val_loss: 1.8411 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3500 - val_top_k_in_k: 0.6645\n",
      "Epoch 65/5000\n",
      "32979/32979 [==============================] - 6s 184us/step - loss: 1.6890 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3635 - top_k_in_k: 0.6906 - val_loss: 1.8393 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3470 - val_top_k_in_k: 0.6651\n",
      "Epoch 66/5000\n",
      "32979/32979 [==============================] - 6s 183us/step - loss: 1.6865 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3636 - top_k_in_k: 0.6916 - val_loss: 1.8371 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3493 - val_top_k_in_k: 0.6658\n",
      "Epoch 67/5000\n",
      "32979/32979 [==============================] - 6s 187us/step - loss: 1.6841 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3632 - top_k_in_k: 0.6924 - val_loss: 1.8357 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3493 - val_top_k_in_k: 0.6671\n",
      "Epoch 68/5000\n",
      "32979/32979 [==============================] - 6s 185us/step - loss: 1.6817 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3645 - top_k_in_k: 0.6937 - val_loss: 1.8337 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3449 - val_top_k_in_k: 0.6676\n",
      "Epoch 69/5000\n",
      "32979/32979 [==============================] - 6s 181us/step - loss: 1.6794 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3617 - top_k_in_k: 0.6944 - val_loss: 1.8320 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3497 - val_top_k_in_k: 0.6685\n",
      "Epoch 70/5000\n",
      "32979/32979 [==============================] - 6s 178us/step - loss: 1.6772 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3627 - top_k_in_k: 0.6956 - val_loss: 1.8304 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3518 - val_top_k_in_k: 0.6692\n",
      "Epoch 71/5000\n",
      "32979/32979 [==============================] - 6s 175us/step - loss: 1.6750 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3628 - top_k_in_k: 0.6962 - val_loss: 1.8288 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3523 - val_top_k_in_k: 0.6703\n",
      "Epoch 72/5000\n",
      "32979/32979 [==============================] - 6s 167us/step - loss: 1.6729 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3626 - top_k_in_k: 0.6973 - val_loss: 1.8273 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3509 - val_top_k_in_k: 0.6711\n",
      "Epoch 73/5000\n",
      "32979/32979 [==============================] - 6s 191us/step - loss: 1.6708 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3634 - top_k_in_k: 0.6981 - val_loss: 1.8257 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3532 - val_top_k_in_k: 0.6718\n",
      "Epoch 74/5000\n",
      "32979/32979 [==============================] - 6s 184us/step - loss: 1.6688 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3634 - top_k_in_k: 0.6990 - val_loss: 1.8245 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3504 - val_top_k_in_k: 0.6726\n",
      "Epoch 75/5000\n",
      "32979/32979 [==============================] - 6s 194us/step - loss: 1.6668 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3631 - top_k_in_k: 0.6998 - val_loss: 1.8232 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3464 - val_top_k_in_k: 0.6730\n",
      "Epoch 76/5000\n",
      "32979/32979 [==============================] - 6s 190us/step - loss: 1.6649 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3617 - top_k_in_k: 0.7004 - val_loss: 1.8219 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3543 - val_top_k_in_k: 0.6746\n",
      "Epoch 77/5000\n",
      "32979/32979 [==============================] - 6s 174us/step - loss: 1.6630 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3650 - top_k_in_k: 0.7014 - val_loss: 1.8207 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3488 - val_top_k_in_k: 0.6749\n",
      "Epoch 78/5000\n",
      "32979/32979 [==============================] - 6s 180us/step - loss: 1.6612 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3623 - top_k_in_k: 0.7021 - val_loss: 1.8192 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3516 - val_top_k_in_k: 0.6759\n",
      "Epoch 79/5000\n",
      "32979/32979 [==============================] - 6s 178us/step - loss: 1.6594 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3628 - top_k_in_k: 0.7031 - val_loss: 1.8177 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3502 - val_top_k_in_k: 0.6758\n",
      "Epoch 80/5000\n",
      "32979/32979 [==============================] - 6s 181us/step - loss: 1.6577 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3636 - top_k_in_k: 0.7035 - val_loss: 1.8167 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3511 - val_top_k_in_k: 0.6768\n",
      "Epoch 81/5000\n",
      "32979/32979 [==============================] - 6s 180us/step - loss: 1.6560 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3612 - top_k_in_k: 0.7043 - val_loss: 1.8155 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3529 - val_top_k_in_k: 0.6783\n",
      "Epoch 82/5000\n",
      "32979/32979 [==============================] - 6s 170us/step - loss: 1.6543 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3629 - top_k_in_k: 0.7052 - val_loss: 1.8144 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3486 - val_top_k_in_k: 0.6777\n",
      "Epoch 83/5000\n",
      "32979/32979 [==============================] - 6s 188us/step - loss: 1.6526 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3607 - top_k_in_k: 0.7056 - val_loss: 1.8135 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3543 - val_top_k_in_k: 0.6788\n",
      "Epoch 84/5000\n",
      "32979/32979 [==============================] - 6s 183us/step - loss: 1.6511 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3638 - top_k_in_k: 0.7065 - val_loss: 1.8124 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3495 - val_top_k_in_k: 0.6791\n",
      "Epoch 85/5000\n",
      "32979/32979 [==============================] - 6s 182us/step - loss: 1.6495 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3616 - top_k_in_k: 0.7070 - val_loss: 1.8111 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3511 - val_top_k_in_k: 0.6803\n",
      "Epoch 86/5000\n",
      "32979/32979 [==============================] - 6s 185us/step - loss: 1.6480 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3630 - top_k_in_k: 0.7077 - val_loss: 1.8103 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3504 - val_top_k_in_k: 0.6810\n",
      "Epoch 87/5000\n",
      "32979/32979 [==============================] - 6s 177us/step - loss: 1.6465 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3616 - top_k_in_k: 0.7084 - val_loss: 1.8093 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3511 - val_top_k_in_k: 0.6813\n",
      "Epoch 88/5000\n",
      "32979/32979 [==============================] - 6s 192us/step - loss: 1.6450 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3631 - top_k_in_k: 0.7090 - val_loss: 1.8084 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3494 - val_top_k_in_k: 0.6819\n",
      "Epoch 89/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32979/32979 [==============================] - 6s 181us/step - loss: 1.6436 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3617 - top_k_in_k: 0.7098 - val_loss: 1.8075 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3474 - val_top_k_in_k: 0.6821\n",
      "Epoch 90/5000\n",
      "32979/32979 [==============================] - 6s 180us/step - loss: 1.6422 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3625 - top_k_in_k: 0.7102 - val_loss: 1.8066 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3417 - val_top_k_in_k: 0.6821\n",
      "Epoch 91/5000\n",
      "32979/32979 [==============================] - 6s 171us/step - loss: 1.6408 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3607 - top_k_in_k: 0.7107 - val_loss: 1.8057 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3523 - val_top_k_in_k: 0.6834\n",
      "Epoch 92/5000\n",
      "32979/32979 [==============================] - 6s 173us/step - loss: 1.6394 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3623 - top_k_in_k: 0.7114 - val_loss: 1.8052 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3511 - val_top_k_in_k: 0.6837\n",
      "Epoch 93/5000\n",
      "32979/32979 [==============================] - 6s 170us/step - loss: 1.6382 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3614 - top_k_in_k: 0.7120 - val_loss: 1.8040 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3498 - val_top_k_in_k: 0.6842\n",
      "Epoch 94/5000\n",
      "32979/32979 [==============================] - 6s 175us/step - loss: 1.6369 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3630 - top_k_in_k: 0.7125 - val_loss: 1.8032 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3497 - val_top_k_in_k: 0.6850\n",
      "Epoch 95/5000\n",
      "32979/32979 [==============================] - 6s 174us/step - loss: 1.6356 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3628 - top_k_in_k: 0.7130 - val_loss: 1.8025 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3478 - val_top_k_in_k: 0.6851\n",
      "Epoch 96/5000\n",
      "32979/32979 [==============================] - 6s 178us/step - loss: 1.6344 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3625 - top_k_in_k: 0.7136 - val_loss: 1.8015 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3481 - val_top_k_in_k: 0.6852\n",
      "Epoch 97/5000\n",
      "32979/32979 [==============================] - 6s 195us/step - loss: 1.6332 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3612 - top_k_in_k: 0.7141 - val_loss: 1.8008 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3481 - val_top_k_in_k: 0.6860\n",
      "Epoch 98/5000\n",
      "32979/32979 [==============================] - 6s 188us/step - loss: 1.6320 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3620 - top_k_in_k: 0.7147 - val_loss: 1.8003 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3464 - val_top_k_in_k: 0.6865\n",
      "Epoch 99/5000\n",
      "32979/32979 [==============================] - 6s 187us/step - loss: 1.6308 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3610 - top_k_in_k: 0.7151 - val_loss: 1.7994 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3482 - val_top_k_in_k: 0.6868\n",
      "Epoch 100/5000\n",
      "32979/32979 [==============================] - 6s 177us/step - loss: 1.6296 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3615 - top_k_in_k: 0.7156 - val_loss: 1.7986 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3464 - val_top_k_in_k: 0.6872\n",
      "Epoch 101/5000\n",
      "32979/32979 [==============================] - 6s 170us/step - loss: 1.6285 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3620 - top_k_in_k: 0.7163 - val_loss: 1.7980 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3482 - val_top_k_in_k: 0.6868\n",
      "Epoch 102/5000\n",
      "32979/32979 [==============================] - 5s 165us/step - loss: 1.6274 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3623 - top_k_in_k: 0.7165 - val_loss: 1.7975 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3449 - val_top_k_in_k: 0.6882\n",
      "Epoch 103/5000\n",
      "32979/32979 [==============================] - 6s 174us/step - loss: 1.6263 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3611 - top_k_in_k: 0.7171 - val_loss: 1.7968 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3459 - val_top_k_in_k: 0.6883\n",
      "Epoch 104/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.6252 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3604 - top_k_in_k: 0.7175 - val_loss: 1.7961 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3508 - val_top_k_in_k: 0.6889\n",
      "Epoch 105/5000\n",
      "32979/32979 [==============================] - 5s 160us/step - loss: 1.6242 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3620 - top_k_in_k: 0.7180 - val_loss: 1.7955 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3506 - val_top_k_in_k: 0.6895\n",
      "Epoch 106/5000\n",
      "32979/32979 [==============================] - 6s 168us/step - loss: 1.6231 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3617 - top_k_in_k: 0.7186 - val_loss: 1.7949 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3476 - val_top_k_in_k: 0.6896\n",
      "Epoch 107/5000\n",
      "32979/32979 [==============================] - 5s 166us/step - loss: 1.6221 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3622 - top_k_in_k: 0.7188 - val_loss: 1.7943 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3424 - val_top_k_in_k: 0.6902\n",
      "Epoch 108/5000\n",
      "32979/32979 [==============================] - 6s 172us/step - loss: 1.6211 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3597 - top_k_in_k: 0.7194 - val_loss: 1.7938 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3503 - val_top_k_in_k: 0.6902\n",
      "Epoch 109/5000\n",
      "32979/32979 [==============================] - 6s 167us/step - loss: 1.6201 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3605 - top_k_in_k: 0.7198 - val_loss: 1.7931 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3469 - val_top_k_in_k: 0.6907\n",
      "Epoch 110/5000\n",
      "32979/32979 [==============================] - 6s 186us/step - loss: 1.6191 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3612 - top_k_in_k: 0.7202 - val_loss: 1.7929 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3448 - val_top_k_in_k: 0.6912\n",
      "Epoch 111/5000\n",
      "32979/32979 [==============================] - 6s 178us/step - loss: 1.6182 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3608 - top_k_in_k: 0.7206 - val_loss: 1.7921 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3415 - val_top_k_in_k: 0.6913\n",
      "Epoch 112/5000\n",
      "32979/32979 [==============================] - 6s 177us/step - loss: 1.6172 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3604 - top_k_in_k: 0.7211 - val_loss: 1.7918 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3457 - val_top_k_in_k: 0.6919\n",
      "Epoch 113/5000\n",
      "32979/32979 [==============================] - 6s 180us/step - loss: 1.6163 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3612 - top_k_in_k: 0.7214 - val_loss: 1.7912 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3459 - val_top_k_in_k: 0.6922\n",
      "Epoch 114/5000\n",
      "32979/32979 [==============================] - 6s 180us/step - loss: 1.6154 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3620 - top_k_in_k: 0.7219 - val_loss: 1.7907 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3424 - val_top_k_in_k: 0.6926\n",
      "Epoch 115/5000\n",
      "32979/32979 [==============================] - 6s 188us/step - loss: 1.6145 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3609 - top_k_in_k: 0.7223 - val_loss: 1.7904 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3475 - val_top_k_in_k: 0.6925\n",
      "Epoch 116/5000\n",
      "32979/32979 [==============================] - 6s 182us/step - loss: 1.6137 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3611 - top_k_in_k: 0.7226 - val_loss: 1.7898 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3453 - val_top_k_in_k: 0.6934\n",
      "Epoch 117/5000\n",
      "32979/32979 [==============================] - 6s 175us/step - loss: 1.6128 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3603 - top_k_in_k: 0.7232 - val_loss: 1.7891 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3449 - val_top_k_in_k: 0.6932\n",
      "Epoch 118/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32979/32979 [==============================] - 6s 176us/step - loss: 1.6119 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3608 - top_k_in_k: 0.7233 - val_loss: 1.7890 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3464 - val_top_k_in_k: 0.6938\n",
      "Epoch 119/5000\n",
      "32979/32979 [==============================] - 6s 168us/step - loss: 1.6111 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3593 - top_k_in_k: 0.7237 - val_loss: 1.7883 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3508 - val_top_k_in_k: 0.6945\n",
      "Epoch 120/5000\n",
      "32979/32979 [==============================] - 5s 166us/step - loss: 1.6103 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3619 - top_k_in_k: 0.7242 - val_loss: 1.7879 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3434 - val_top_k_in_k: 0.6943\n",
      "Epoch 121/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.6094 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3606 - top_k_in_k: 0.7245 - val_loss: 1.7876 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3454 - val_top_k_in_k: 0.6947\n",
      "Epoch 122/5000\n",
      "32979/32979 [==============================] - 5s 157us/step - loss: 1.6087 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3612 - top_k_in_k: 0.7249 - val_loss: 1.7872 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3457 - val_top_k_in_k: 0.6952\n",
      "Epoch 123/5000\n",
      "32979/32979 [==============================] - 6s 171us/step - loss: 1.6078 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3589 - top_k_in_k: 0.7251 - val_loss: 1.7868 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3515 - val_top_k_in_k: 0.6960\n",
      "Epoch 124/5000\n",
      "32979/32979 [==============================] - 5s 161us/step - loss: 1.6071 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3607 - top_k_in_k: 0.7255 - val_loss: 1.7865 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3442 - val_top_k_in_k: 0.6961\n",
      "Epoch 125/5000\n",
      "32979/32979 [==============================] - 5s 156us/step - loss: 1.6063 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3612 - top_k_in_k: 0.7260 - val_loss: 1.7858 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3435 - val_top_k_in_k: 0.6961\n",
      "Epoch 126/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.6056 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3596 - top_k_in_k: 0.7261 - val_loss: 1.7855 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3475 - val_top_k_in_k: 0.6970\n",
      "Epoch 127/5000\n",
      "32979/32979 [==============================] - 6s 168us/step - loss: 1.6048 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3614 - top_k_in_k: 0.7267 - val_loss: 1.7853 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3463 - val_top_k_in_k: 0.6967\n",
      "Epoch 128/5000\n",
      "32979/32979 [==============================] - 6s 179us/step - loss: 1.6041 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3597 - top_k_in_k: 0.7269 - val_loss: 1.7851 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3472 - val_top_k_in_k: 0.6970\n",
      "Epoch 129/5000\n",
      "32979/32979 [==============================] - 5s 165us/step - loss: 1.6034 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3599 - top_k_in_k: 0.7273 - val_loss: 1.7847 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3448 - val_top_k_in_k: 0.6970\n",
      "Epoch 130/5000\n",
      "32979/32979 [==============================] - 6s 168us/step - loss: 1.6027 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3596 - top_k_in_k: 0.7276 - val_loss: 1.7841 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3460 - val_top_k_in_k: 0.6967\n",
      "Epoch 131/5000\n",
      "32979/32979 [==============================] - 5s 166us/step - loss: 1.6019 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3590 - top_k_in_k: 0.7277 - val_loss: 1.7837 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3476 - val_top_k_in_k: 0.6976\n",
      "Epoch 132/5000\n",
      "32979/32979 [==============================] - 5s 165us/step - loss: 1.6012 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3596 - top_k_in_k: 0.7281 - val_loss: 1.7836 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3463 - val_top_k_in_k: 0.6981\n",
      "Epoch 133/5000\n",
      "32979/32979 [==============================] - 6s 167us/step - loss: 1.6006 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3606 - top_k_in_k: 0.7286 - val_loss: 1.7832 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3426 - val_top_k_in_k: 0.6980\n",
      "Epoch 134/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.5999 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3589 - top_k_in_k: 0.7288 - val_loss: 1.7828 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3461 - val_top_k_in_k: 0.6976\n",
      "Epoch 135/5000\n",
      "32979/32979 [==============================] - 6s 169us/step - loss: 1.5992 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3610 - top_k_in_k: 0.7289 - val_loss: 1.7826 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3442 - val_top_k_in_k: 0.6984\n",
      "Epoch 136/5000\n",
      "32979/32979 [==============================] - 5s 167us/step - loss: 1.5985 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3593 - top_k_in_k: 0.7295 - val_loss: 1.7822 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3463 - val_top_k_in_k: 0.6987\n",
      "Epoch 137/5000\n",
      "32979/32979 [==============================] - 6s 172us/step - loss: 1.5979 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3613 - top_k_in_k: 0.7296 - val_loss: 1.7818 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3432 - val_top_k_in_k: 0.6991\n",
      "Epoch 138/5000\n",
      "32979/32979 [==============================] - 5s 164us/step - loss: 1.5972 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3598 - top_k_in_k: 0.7298 - val_loss: 1.7817 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3468 - val_top_k_in_k: 0.6996\n",
      "Epoch 139/5000\n",
      "32979/32979 [==============================] - 6s 167us/step - loss: 1.5966 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3599 - top_k_in_k: 0.7303 - val_loss: 1.7812 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3469 - val_top_k_in_k: 0.6997\n",
      "Epoch 140/5000\n",
      "32979/32979 [==============================] - 6s 175us/step - loss: 1.5960 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3593 - top_k_in_k: 0.7305 - val_loss: 1.7810 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3421 - val_top_k_in_k: 0.7001\n",
      "Epoch 141/5000\n",
      "32979/32979 [==============================] - 6s 172us/step - loss: 1.5954 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3582 - top_k_in_k: 0.7308 - val_loss: 1.7808 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3461 - val_top_k_in_k: 0.7002\n",
      "Epoch 142/5000\n",
      "32979/32979 [==============================] - 5s 165us/step - loss: 1.5947 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3590 - top_k_in_k: 0.7310 - val_loss: 1.7806 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3449 - val_top_k_in_k: 0.7002\n",
      "Epoch 143/5000\n",
      "32979/32979 [==============================] - 5s 167us/step - loss: 1.5942 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3603 - top_k_in_k: 0.7313 - val_loss: 1.7804 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3428 - val_top_k_in_k: 0.7008\n",
      "Epoch 144/5000\n",
      "32979/32979 [==============================] - 5s 159us/step - loss: 1.5936 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3601 - top_k_in_k: 0.7316 - val_loss: 1.7802 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3438 - val_top_k_in_k: 0.7004\n",
      "Epoch 145/5000\n",
      "32979/32979 [==============================] - 5s 156us/step - loss: 1.5930 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3596 - top_k_in_k: 0.7319 - val_loss: 1.7799 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3426 - val_top_k_in_k: 0.7006\n",
      "Epoch 146/5000\n",
      "32979/32979 [==============================] - 5s 149us/step - loss: 1.5924 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3598 - top_k_in_k: 0.7320 - val_loss: 1.7794 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3454 - val_top_k_in_k: 0.7013\n",
      "Epoch 147/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32979/32979 [==============================] - 5s 149us/step - loss: 1.5918 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3611 - top_k_in_k: 0.7322 - val_loss: 1.7792 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3424 - val_top_k_in_k: 0.7020\n",
      "Epoch 148/5000\n",
      "32979/32979 [==============================] - 5s 161us/step - loss: 1.5913 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3580 - top_k_in_k: 0.7328 - val_loss: 1.7791 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3437 - val_top_k_in_k: 0.7018\n",
      "Epoch 149/5000\n",
      "32979/32979 [==============================] - 5s 157us/step - loss: 1.5907 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3597 - top_k_in_k: 0.7329 - val_loss: 1.7789 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3491 - val_top_k_in_k: 0.7022\n",
      "Epoch 150/5000\n",
      "32979/32979 [==============================] - 5s 149us/step - loss: 1.5902 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3598 - top_k_in_k: 0.7332 - val_loss: 1.7786 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3441 - val_top_k_in_k: 0.7019\n",
      "Epoch 151/5000\n",
      "32979/32979 [==============================] - 6s 171us/step - loss: 1.5896 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3589 - top_k_in_k: 0.7334 - val_loss: 1.7783 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3430 - val_top_k_in_k: 0.7023\n",
      "Epoch 152/5000\n",
      "32979/32979 [==============================] - 5s 166us/step - loss: 1.5891 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3588 - top_k_in_k: 0.7338 - val_loss: 1.7782 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3435 - val_top_k_in_k: 0.7020\n",
      "Epoch 153/5000\n",
      "32979/32979 [==============================] - 6s 171us/step - loss: 1.5886 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3595 - top_k_in_k: 0.7337 - val_loss: 1.7777 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3442 - val_top_k_in_k: 0.7031\n",
      "Epoch 154/5000\n",
      "32979/32979 [==============================] - 6s 167us/step - loss: 1.5880 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3589 - top_k_in_k: 0.7342 - val_loss: 1.7779 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3440 - val_top_k_in_k: 0.7030\n",
      "Epoch 155/5000\n",
      "32979/32979 [==============================] - 6s 171us/step - loss: 1.5875 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3599 - top_k_in_k: 0.7344 - val_loss: 1.7774 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3441 - val_top_k_in_k: 0.7029\n",
      "Epoch 156/5000\n",
      "32979/32979 [==============================] - 5s 158us/step - loss: 1.5870 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3587 - top_k_in_k: 0.7346 - val_loss: 1.7773 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3437 - val_top_k_in_k: 0.7032\n",
      "Epoch 157/5000\n",
      "32979/32979 [==============================] - 5s 161us/step - loss: 1.5865 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3597 - top_k_in_k: 0.7349 - val_loss: 1.7771 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3442 - val_top_k_in_k: 0.7029\n",
      "Epoch 158/5000\n",
      "32979/32979 [==============================] - 5s 154us/step - loss: 1.5860 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3599 - top_k_in_k: 0.7349 - val_loss: 1.7771 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3408 - val_top_k_in_k: 0.7032\n",
      "Epoch 159/5000\n",
      "32979/32979 [==============================] - 5s 163us/step - loss: 1.5855 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3596 - top_k_in_k: 0.7353 - val_loss: 1.7768 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3391 - val_top_k_in_k: 0.7037\n",
      "Epoch 160/5000\n",
      "32979/32979 [==============================] - 5s 158us/step - loss: 1.5850 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3565 - top_k_in_k: 0.7355 - val_loss: 1.7765 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3459 - val_top_k_in_k: 0.7040\n",
      "Epoch 161/5000\n",
      "32979/32979 [==============================] - 5s 166us/step - loss: 1.5845 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3599 - top_k_in_k: 0.7357 - val_loss: 1.7764 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3454 - val_top_k_in_k: 0.7039\n",
      "Epoch 162/5000\n",
      "32979/32979 [==============================] - 5s 160us/step - loss: 1.5841 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3590 - top_k_in_k: 0.7361 - val_loss: 1.7762 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3418 - val_top_k_in_k: 0.7041\n",
      "Epoch 163/5000\n",
      "32979/32979 [==============================] - 5s 163us/step - loss: 1.5835 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3589 - top_k_in_k: 0.7361 - val_loss: 1.7763 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3443 - val_top_k_in_k: 0.7045\n",
      "Epoch 164/5000\n",
      "32979/32979 [==============================] - 6s 182us/step - loss: 1.5831 - binary_accuracy: 0.9870 - categorical_accuracy: 0.3593 - top_k_in_k: 0.7363 - val_loss: 1.7762 - val_binary_accuracy: 0.9870 - val_categorical_accuracy: 0.3438 - val_top_k_in_k: 0.7053\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "#basic sequenctial keras NN model with 2 layers.\n",
    "#Input are vectors of strategies that participate in the tournament\n",
    "#Output is prediction of 1 hot vector representing the 3 most dominant strategies in the tournament\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(units=231, activation='softmax', input_dim=3234))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.001),\n",
    "              metrics=['binary_accuracy', 'categorical_accuracy', top_k_in_k])\n",
    "\n",
    "history2 = model2.fit(np.array(x_train), np.array(y_train), \n",
    "                    validation_split=0.2, \n",
    "                    epochs=5000, \n",
    "                    verbose=1, \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')],\n",
    "                    batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history2.history2.keys())\n",
    "#  \"Accuracy\"\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history2.history['binary_accuracy'])\n",
    "plt.plot(history2.history['val_binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history2.history['categorical_accuracy'])\n",
    "plt.plot(history2.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred2 = model2.predict(x_train[:n])\n",
    "\n",
    "top_k_in_k(y_true, y_pred2)\n",
    "\n",
    "model2.save('logis_model_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
